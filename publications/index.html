<!DOCTYPE html><html lang="en" class="scroll-smooth"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="stylesheet" href="/_next/static/css/19bf7477d82f620e.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/_next/static/chunks/webpack-679b75d1c4c2027c.js"/><script src="/_next/static/chunks/4bd1b696-70b6d399998de86a.js" async=""></script><script src="/_next/static/chunks/684-5aaa8290a129f299.js" async=""></script><script src="/_next/static/chunks/main-app-0f9a19a0d5325b39.js" async=""></script><script src="/_next/static/chunks/17-9c04c9413a9f9f1f.js" async=""></script><script src="/_next/static/chunks/874-6cc630662f3664af.js" async=""></script><script src="/_next/static/chunks/862-15038af301665bb3.js" async=""></script><script src="/_next/static/chunks/app/layout-347869ba10f90412.js" async=""></script><script src="/_next/static/chunks/178-595a94b9af1e67b5.js" async=""></script><script src="/_next/static/chunks/748-1f3129a1e6365cf9.js" async=""></script><script src="/_next/static/chunks/app/%5Bslug%5D/page-78388292e0713c46.js" async=""></script><link rel="icon" href="/favicon.svg" type="image/svg+xml"/><link rel="dns-prefetch" href="https://google-fonts.jialeliu.com"/><link rel="preconnect" href="https://google-fonts.jialeliu.com" crossorigin=""/><link rel="preload" as="style" href="https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700&amp;family=Crimson+Text:ital,wght@0,400;0,600;1,400&amp;display=swap"/><link rel="stylesheet" id="gfonts-css" href="https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700&amp;family=Crimson+Text:ital,wght@0,400;0,600;1,400&amp;display=swap" media="print"/><script>
              (function(){
                var l = document.getElementById('gfonts-css');
                if (!l) return;
                if (l.media !== 'all') {
                  l.addEventListener('load', function(){ try { l.media = 'all'; } catch(e){} });
                }
              })();
            </script><noscript><link rel="stylesheet" href="https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700&amp;family=Crimson+Text:ital,wght@0,400;0,600;1,400&amp;display=swap"/></noscript><script>
              try {
                const theme = localStorage.getItem('theme-storage');
                const parsed = theme ? JSON.parse(theme) : null;
                const setting = parsed?.state?.theme || 'system';
                const prefersDark = typeof window !== 'undefined' && window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches;
                const effective = setting === 'dark' ? 'dark' : (setting === 'light' ? 'light' : (prefersDark ? 'dark' : 'light'));
                var root = document.documentElement;
                root.classList.add(effective);
                root.setAttribute('data-theme', effective);
              } catch (e) {
                var root = document.documentElement;
                root.classList.add('light');
                root.setAttribute('data-theme', 'light');
              }
            </script><title>Publications | Zhengzhuo Xu (许正卓)</title><meta name="description" content="A collection of my research work."/><meta name="author" content="Zhengzhuo Xu"/><meta name="keywords" content="Zhengzhuo Xu,PhD,Research,CS Ph.D. Candidate"/><meta name="creator" content="Zhengzhuo Xu"/><meta name="publisher" content="Zhengzhuo Xu"/><meta property="og:title" content="Zhengzhuo Xu (许正卓)"/><meta property="og:description" content="PhD student at the University of Example."/><meta property="og:site_name" content="Zhengzhuo Xu&#x27;s Academic Website"/><meta property="og:locale" content="en_US"/><meta property="og:type" content="website"/><meta name="twitter:card" content="summary"/><meta name="twitter:title" content="Zhengzhuo Xu (许正卓)"/><meta name="twitter:description" content="PhD student at the University of Example."/><link rel="icon" href="/favicon.svg"/><script>document.querySelectorAll('body link[rel="icon"], body link[rel="apple-touch-icon"]').forEach(el => document.head.appendChild(el))</script><script src="/_next/static/chunks/polyfills-42372ed130431b0a.js" noModule=""></script></head><body class="font-sans antialiased"><div style="visibility:hidden"><nav class="fixed top-0 left-0 right-0 z-50" data-headlessui-state=""><div class="transition-all duration-300 ease-out bg-transparent" style="transform:translateY(-100px)"><div class="max-w-7xl mx-auto px-4 sm:px-6 lg:px-8"><div class="flex justify-between items-center h-16 lg:h-20"><div class="flex-shrink-0" tabindex="0"><a class="text-xl lg:text-2xl font-serif font-semibold text-primary hover:text-accent transition-colors duration-200" href="/">Zhengzhuo Xu (许正卓)</a></div><div class="hidden lg:block"><div class="ml-10 flex items-center space-x-8"><div class="flex items-baseline space-x-8"><a class="relative px-3 py-2 text-sm font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm text-neutral-600 hover:text-primary" href="/"><span class="relative z-10">About</span></a><a class="relative px-3 py-2 text-sm font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm text-neutral-600 hover:text-primary" href="/education/"><span class="relative z-10">Education</span></a><a class="relative px-3 py-2 text-sm font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm text-primary" href="/publications/"><span class="relative z-10">Publications</span><div class="absolute inset-0 bg-accent/10 rounded-lg"></div></a><a class="relative px-3 py-2 text-sm font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm text-neutral-600 hover:text-primary" href="/career/"><span class="relative z-10">Career</span></a><a class="relative px-3 py-2 text-sm font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm text-neutral-600 hover:text-primary" href="/awards/"><span class="relative z-10">Awards</span></a></div><div class="flex items-center justify-center w-10 h-10 rounded-lg border border-neutral-200 dark:border-[rgba(148,163,184,0.24)] bg-background dark:bg-neutral-800"><div class="w-4 h-4 rounded-full bg-neutral-300 animate-pulse"></div></div></div></div><div class="lg:hidden flex items-center space-x-2"><div class="flex items-center justify-center w-10 h-10 rounded-lg border border-neutral-200 dark:border-[rgba(148,163,184,0.24)] bg-background dark:bg-neutral-800"><div class="w-4 h-4 rounded-full bg-neutral-300 animate-pulse"></div></div><button class="inline-flex items-center justify-center p-2 rounded-md text-neutral-600 hover:text-primary hover:bg-neutral-100 dark:hover:bg-neutral-800 focus:outline-none focus:ring-2 focus:ring-inset focus:ring-accent transition-colors duration-200" id="headlessui-disclosure-button-«R5pdb»" type="button" aria-expanded="false" data-headlessui-state=""><span class="sr-only">Open main menu</span><div><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="block h-6 w-6"><path stroke-linecap="round" stroke-linejoin="round" d="M3.75 6.75h16.5M3.75 12h16.5m-16.5 5.25h16.5"></path></svg></div></button></div></div></div></div></nav><main class="min-h-screen pt-16 lg:pt-20"><div class="max-w-4xl mx-auto px-4 sm:px-6 lg:px-8 py-12"><div style="opacity:0;transform:translateY(20px)"><div class="mb-8"><h1 class="text-4xl font-serif font-bold text-primary mb-4">Publications</h1><p class="text-lg text-neutral-600 dark:text-neutral-500 max-w-2xl">A collection of my research work.</p></div><div class="mb-8 space-y-4"><div class="flex flex-col sm:flex-row gap-4"><div class="relative flex-grow"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="absolute left-3 top-1/2 transform -translate-y-1/2 h-5 w-5 text-neutral-400"><path stroke-linecap="round" stroke-linejoin="round" d="m21 21-5.197-5.197m0 0A7.5 7.5 0 1 0 5.196 5.196a7.5 7.5 0 0 0 10.607 10.607Z"></path></svg><input type="text" placeholder="Search publications..." class="w-full pl-10 pr-4 py-2 rounded-lg border border-neutral-200 dark:border-neutral-800 bg-white dark:bg-neutral-900 focus:ring-2 focus:ring-accent focus:border-transparent transition-all duration-200" value=""/></div><button class="flex items-center justify-center px-4 py-2 rounded-lg border transition-all duration-200 bg-white dark:bg-neutral-900 border-neutral-200 dark:border-neutral-800 text-neutral-600 hover:border-accent hover:text-accent"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-5 w-5 mr-2"><path stroke-linecap="round" stroke-linejoin="round" d="M12 3c2.755 0 5.455.232 8.083.678.533.09.917.556.917 1.096v1.044a2.25 2.25 0 0 1-.659 1.591l-5.432 5.432a2.25 2.25 0 0 0-.659 1.591v2.927a2.25 2.25 0 0 1-1.244 2.013L9.75 21v-6.568a2.25 2.25 0 0 0-.659-1.591L3.659 7.409A2.25 2.25 0 0 1 3 5.818V4.774c0-.54.384-1.006.917-1.096A48.32 48.32 0 0 1 12 3Z"></path></svg>Filters</button></div></div><div class="space-y-6"><div class="bg-white dark:bg-neutral-900 p-6 rounded-xl shadow-sm border border-neutral-200 dark:border-neutral-800 hover:shadow-md transition-all duration-200" style="opacity:0;transform:translateY(20px)"><div class="flex flex-col md:flex-row gap-6"><div class="w-full md:w-48 flex-shrink-0"><div class="aspect-video md:aspect-[4/3] relative rounded-lg overflow-hidden bg-neutral-100 dark:bg-neutral-800"><img alt="VACoT: Rethinking Visual Data Augmentation with VLMs" loading="lazy" decoding="async" data-nimg="fill" class="object-cover" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;color:transparent" src="/papers/VACoT.png"/></div></div><div class="flex-grow"><h3 class="text-xl font-semibold text-primary mb-2 leading-tight">VACoT: Rethinking Visual Data Augmentation with VLMs</h3><p class="text-base text-neutral-600 dark:text-neutral-400 mb-2"><span><span class="font-semibold text-accent">Zhengzhuo Xu</span>, </span><span><span class="">Chong Sun</span>, </span><span><span class="">SiNan Du</span>, </span><span><span class="">Chen Li</span>, </span><span><span class="">Jing LYU</span>, </span><span><span class="">Chun Yuan</span></span></p><p class="text-sm font-medium text-neutral-800 dark:text-neutral-600 mb-3">Arxiv preprint<!-- --> <!-- -->2025</p><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-4 line-clamp-3">A research on Visual Augmentation Chain-of-Thought, a framework that dynamically applies visual augmentations during VLM inference to enhance robustness on challenging/out-of-distribution inputs. It integrates diverse general augmentations (beyond local cropping) with efficient agentic reinforcement learning &amp; conditional rewards, achieving superior performance on 13 perception benchmarks and introducing AdvOCR to verify post-hoc augmentation benefits.</p><div class="flex flex-wrap gap-2 mt-auto"><a href="https://doi.org/10.48550/ARXIV.2512.02361" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">DOI</a><button class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium transition-colors bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M12 6.042A8.967 8.967 0 0 0 6 3.75c-1.052 0-2.062.18-3 .512v14.25A8.987 8.987 0 0 1 6 18c2.305 0 4.408.867 6 2.292m0-14.25a8.966 8.966 0 0 1 6-2.292c1.052 0 2.062.18 3 .512v14.25A8.987 8.987 0 0 0 18 18a8.967 8.967 0 0 0-6 2.292m0-14.25v14.25"></path></svg>BibTeX</button></div></div></div></div><div class="bg-white dark:bg-neutral-900 p-6 rounded-xl shadow-sm border border-neutral-200 dark:border-neutral-800 hover:shadow-md transition-all duration-200" style="opacity:0;transform:translateY(20px)"><div class="flex flex-col md:flex-row gap-6"><div class="w-full md:w-48 flex-shrink-0"><div class="aspect-video md:aspect-[4/3] relative rounded-lg overflow-hidden bg-neutral-100 dark:bg-neutral-800"><img alt="VQRAE: Representation Quantization Autoencoders for Multimodal Understanding, Generation and Reconstruction" loading="lazy" decoding="async" data-nimg="fill" class="object-cover" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;color:transparent" src="/papers/VQRAE.png"/></div></div><div class="flex-grow"><h3 class="text-xl font-semibold text-primary mb-2 leading-tight">VQRAE: Representation Quantization Autoencoders for Multimodal Understanding, Generation and Reconstruction</h3><p class="text-base text-neutral-600 dark:text-neutral-400 mb-2"><span><span class="">Sinan Du</span>, </span><span><span class="">Jiahao Guo</span>, </span><span><span class="">Bo Li</span>, </span><span><span class="">Shuhao Cui</span>, </span><span><span class="font-semibold text-accent">Zhengzhuo Xu</span>, </span><span><span class="">Yifu Luo</span>, </span><span><span class="">Yongxian Wei</span>, </span><span><span class="">Kun Gai</span>, </span><span><span class="">Xinggang Wang</span>, </span><span><span class="">Kai Wu</span>, </span><span><span class="">Chun Yuan</span></span></p><p class="text-sm font-medium text-neutral-800 dark:text-neutral-600 mb-3">Arxiv preprint<!-- --> <!-- -->2025</p><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-4 line-clamp-3">A research on VQRAE, a Vector Quantization-based Representation AutoEncoder that unifies multimodal understanding, generation and reconstruction in a single tokenizer: it produces continuous semantic features for image understanding and discrete tokens for visual generation via a two-stage training strategy (high-dimensional VQ codebook learning + encoder self-distillation), achieving high codebook utilization and competitive performance across visual tasks.</p><div class="flex flex-wrap gap-2 mt-auto"><a href="https://doi.org/10.48550/ARXIV.2511.23386" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">DOI</a><button class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium transition-colors bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M12 6.042A8.967 8.967 0 0 0 6 3.75c-1.052 0-2.062.18-3 .512v14.25A8.987 8.987 0 0 1 6 18c2.305 0 4.408.867 6 2.292m0-14.25a8.966 8.966 0 0 1 6-2.292c1.052 0 2.062.18 3 .512v14.25A8.987 8.987 0 0 0 18 18a8.967 8.967 0 0 0-6 2.292m0-14.25v14.25"></path></svg>BibTeX</button></div></div></div></div><div class="bg-white dark:bg-neutral-900 p-6 rounded-xl shadow-sm border border-neutral-200 dark:border-neutral-800 hover:shadow-md transition-all duration-200" style="opacity:0;transform:translateY(20px)"><div class="flex flex-col md:flex-row gap-6"><div class="w-full md:w-48 flex-shrink-0"><div class="aspect-video md:aspect-[4/3] relative rounded-lg overflow-hidden bg-neutral-100 dark:bg-neutral-800"><img alt="ChartPoint: Guiding MLLMs with Grounding Reflection for Chart Reasoning" loading="lazy" decoding="async" data-nimg="fill" class="object-cover" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;color:transparent" src="/papers/ChartPoint.png"/></div></div><div class="flex-grow"><h3 class="text-xl font-semibold text-primary mb-2 leading-tight">ChartPoint: Guiding MLLMs with Grounding Reflection for Chart Reasoning</h3><p class="text-base text-neutral-600 dark:text-neutral-400 mb-2"><span><span class="font-semibold text-accent">Zhengzhuo Xu</span>, </span><span><span class="">Sinan Du</span>, </span><span><span class="">Yiyan Qi</span>, </span><span><span class="">Siwen Lu</span>, </span><span><span class="">Chengjin Xu</span>, </span><span><span class="">Chun Yuan</span>, </span><span><span class="">Jian Guo</span></span></p><p class="text-sm font-medium text-neutral-800 dark:text-neutral-600 mb-3">IEEE/CVF International Conference on Computer Vision, ICCV<!-- --> <!-- -->2025</p><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-4 line-clamp-3">A research on PointCoT, a framework integrating reflective interaction into chain-of-thought reasoning, designed to strengthen multimodal large language models&#x27; chart comprehension by linking textual reasoning with visual grounding, addressing numerical hallucinations and weak element grounding issues.</p><div class="flex flex-wrap gap-2 mt-auto"><button class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium transition-colors bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M12 6.042A8.967 8.967 0 0 0 6 3.75c-1.052 0-2.062.18-3 .512v14.25A8.987 8.987 0 0 1 6 18c2.305 0 4.408.867 6 2.292m0-14.25a8.966 8.966 0 0 1 6-2.292c1.052 0 2.062.18 3 .512v14.25A8.987 8.987 0 0 0 18 18a8.967 8.967 0 0 0-6 2.292m0-14.25v14.25"></path></svg>BibTeX</button></div></div></div></div><div class="bg-white dark:bg-neutral-900 p-6 rounded-xl shadow-sm border border-neutral-200 dark:border-neutral-800 hover:shadow-md transition-all duration-200" style="opacity:0;transform:translateY(20px)"><div class="flex flex-col md:flex-row gap-6"><div class="w-full md:w-48 flex-shrink-0"><div class="aspect-video md:aspect-[4/3] relative rounded-lg overflow-hidden bg-neutral-100 dark:bg-neutral-800"><img alt="Seed1.5-VL Technical Report" loading="lazy" decoding="async" data-nimg="fill" class="object-cover" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;color:transparent" src="/papers/Seed1.5VL.png"/></div></div><div class="flex-grow"><h3 class="text-xl font-semibold text-primary mb-2 leading-tight">Seed1.5-VL Technical Report</h3><p class="text-base text-neutral-600 dark:text-neutral-400 mb-2"><span><span class="">Dong Guo</span>, </span><span><span class="">Faming Wu</span>, </span><span><span class="">Feida Zhu</span>, </span><span><span class="">Fuxing Leng</span>, </span><span><span class="">Guang Shi</span>, </span><span><span class="">Haobin Chen</span>, </span><span><span class="">Haoqi Fan</span>, </span><span><span class="">Jian Wang</span>, </span><span><span class="">Jianyu Jiang</span>, </span><span><span class="">Jiawei Wang</span>, </span><span><span class="font-semibold text-accent">Zhengzhuo Xu</span>, </span><span><span class="">others</span></span></p><p class="text-sm font-medium text-neutral-800 dark:text-neutral-600 mb-3">Arxiv preprint<!-- --> <!-- -->2025</p><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-4 line-clamp-3">A technical report on Seed1.5-VL, a multimodal model focusing on visual-language tasks, detailing the model architecture, training strategies, and performance evaluations across diverse multimodal benchmarks.</p><div class="flex flex-wrap gap-2 mt-auto"><a href="https://doi.org/10.48550/ARXIV.2505.07062" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">DOI</a><button class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium transition-colors bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M12 6.042A8.967 8.967 0 0 0 6 3.75c-1.052 0-2.062.18-3 .512v14.25A8.987 8.987 0 0 1 6 18c2.305 0 4.408.867 6 2.292m0-14.25a8.966 8.966 0 0 1 6-2.292c1.052 0 2.062.18 3 .512v14.25A8.987 8.987 0 0 0 18 18a8.967 8.967 0 0 0-6 2.292m0-14.25v14.25"></path></svg>BibTeX</button></div></div></div></div><div class="bg-white dark:bg-neutral-900 p-6 rounded-xl shadow-sm border border-neutral-200 dark:border-neutral-800 hover:shadow-md transition-all duration-200" style="opacity:0;transform:translateY(20px)"><div class="flex flex-col md:flex-row gap-6"><div class="w-full md:w-48 flex-shrink-0"><div class="aspect-video md:aspect-[4/3] relative rounded-lg overflow-hidden bg-neutral-100 dark:bg-neutral-800"><img alt="ChartMoE: Mixture of Expert Connector for Advanced Chart Understanding" loading="lazy" decoding="async" data-nimg="fill" class="object-cover" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;color:transparent" src="/papers/ChartMoE.png"/></div></div><div class="flex-grow"><h3 class="text-xl font-semibold text-primary mb-2 leading-tight">ChartMoE: Mixture of Expert Connector for Advanced Chart Understanding</h3><p class="text-base text-neutral-600 dark:text-neutral-400 mb-2"><span><span class="font-semibold text-accent">Zhengzhuo Xu</span>, </span><span><span class="">Bowen Qu</span>, </span><span><span class="">Yiyan Qi</span>, </span><span><span class="">Sinan Du</span>, </span><span><span class="">Chengjin Xu</span>, </span><span><span class="">Chun Yuan</span>, </span><span><span class="">Jian Guo</span></span></p><p class="text-sm font-medium text-neutral-800 dark:text-neutral-600 mb-3">International Conference on Learning Representations, ICLR oral<!-- --> <!-- -->2025</p><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-4 line-clamp-3">A research on ChartMoE, a Mixture of Expert Connector framework designed to enhance advanced chart understanding capabilities, leveraging expert specialization to address complex visual and semantic reasoning challenges in chart data analysis.</p><div class="flex flex-wrap gap-2 mt-auto"><button class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium transition-colors bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M12 6.042A8.967 8.967 0 0 0 6 3.75c-1.052 0-2.062.18-3 .512v14.25A8.987 8.987 0 0 1 6 18c2.305 0 4.408.867 6 2.292m0-14.25a8.966 8.966 0 0 1 6-2.292c1.052 0 2.062.18 3 .512v14.25A8.987 8.987 0 0 0 18 18a8.967 8.967 0 0 0-6 2.292m0-14.25v14.25"></path></svg>BibTeX</button></div></div></div></div><div class="bg-white dark:bg-neutral-900 p-6 rounded-xl shadow-sm border border-neutral-200 dark:border-neutral-800 hover:shadow-md transition-all duration-200" style="opacity:0;transform:translateY(20px)"><div class="flex flex-col md:flex-row gap-6"><div class="w-full md:w-48 flex-shrink-0"><div class="aspect-video md:aspect-[4/3] relative rounded-lg overflow-hidden bg-neutral-100 dark:bg-neutral-800"><img alt="Boosting Long-Tailed Recognition With Label Descriptor and Beyond" loading="lazy" decoding="async" data-nimg="fill" class="object-cover" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;color:transparent" src="/papers/HLC.png"/></div></div><div class="flex-grow"><h3 class="text-xl font-semibold text-primary mb-2 leading-tight">Boosting Long-Tailed Recognition With Label Descriptor and Beyond</h3><p class="text-base text-neutral-600 dark:text-neutral-400 mb-2"><span><span class="font-semibold text-accent">Zhengzhuo Xu</span>, </span><span><span class="">Ruikang Liu</span>, </span><span><span class="">Zenghao Chai</span>, </span><span><span class="">Yiyan Qi</span>, </span><span><span class="">Lei Li</span>, </span><span><span class="">Haiqin Yang</span>, </span><span><span class="">Chun Yuan</span></span></p><p class="text-sm font-medium text-neutral-800 dark:text-neutral-600 mb-3">IEEE Trans. Multim.<!-- --> <!-- -->2025</p><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-4 line-clamp-3">A study focusing on optimizing long-tailed recognition tasks, introducing label descriptor - based technologies and other auxiliary means to alleviate the impact of data class imbalance on model training, which is of great significance for promoting the practical application of recognition models in real - world scenarios with unbalanced data distribution.</p><div class="flex flex-wrap gap-2 mt-auto"><a href="https://doi.org/10.1109/TMM.2025.3607812" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">DOI</a><button class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium transition-colors bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M12 6.042A8.967 8.967 0 0 0 6 3.75c-1.052 0-2.062.18-3 .512v14.25A8.987 8.987 0 0 1 6 18c2.305 0 4.408.867 6 2.292m0-14.25a8.966 8.966 0 0 1 6-2.292c1.052 0 2.062.18 3 .512v14.25A8.987 8.987 0 0 0 18 18a8.967 8.967 0 0 0-6 2.292m0-14.25v14.25"></path></svg>BibTeX</button></div></div></div></div><div class="bg-white dark:bg-neutral-900 p-6 rounded-xl shadow-sm border border-neutral-200 dark:border-neutral-800 hover:shadow-md transition-all duration-200" style="opacity:0;transform:translateY(20px)"><div class="flex flex-col md:flex-row gap-6"><div class="w-full md:w-48 flex-shrink-0"><div class="aspect-video md:aspect-[4/3] relative rounded-lg overflow-hidden bg-neutral-100 dark:bg-neutral-800"><img alt="ALoRE: Efficient Visual Adaptation via Aggregating Low Rank Experts" loading="lazy" decoding="async" data-nimg="fill" class="object-cover" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;color:transparent" src="/papers/ALore.png"/></div></div><div class="flex-grow"><h3 class="text-xl font-semibold text-primary mb-2 leading-tight">ALoRE: Efficient Visual Adaptation via Aggregating Low Rank Experts</h3><p class="text-base text-neutral-600 dark:text-neutral-400 mb-2"><span><span class="">Sinan Du</span>, </span><span><span class="">Guosheng Zhang</span>, </span><span><span class="">Keyao Wang</span>, </span><span><span class="">Yuanrui Wang</span>, </span><span><span class="">Haixiao Yue</span>, </span><span><span class="">Gang Zhang</span>, </span><span><span class="">Errui Ding</span>, </span><span><span class="">Jingdong Wang</span>, </span><span><span class="font-semibold text-accent">Zhengzhuo Xu</span>, </span><span><span class="">Chun Yuan</span></span></p><p class="text-sm font-medium text-neutral-800 dark:text-neutral-600 mb-3">Arxiv preprint<!-- --> <!-- -->2024</p><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-4 line-clamp-3">A research on ALoRE framework for efficient visual adaptation, proposing to aggregate low rank experts to reduce the computational overhead of visual model adaptation while maintaining performance in computer vision tasks.</p><div class="flex flex-wrap gap-2 mt-auto"><a href="https://doi.org/10.48550/ARXIV.2412.08341" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">DOI</a><button class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium transition-colors bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M12 6.042A8.967 8.967 0 0 0 6 3.75c-1.052 0-2.062.18-3 .512v14.25A8.987 8.987 0 0 1 6 18c2.305 0 4.408.867 6 2.292m0-14.25a8.966 8.966 0 0 1 6-2.292c1.052 0 2.062.18 3 .512v14.25A8.987 8.987 0 0 0 18 18a8.967 8.967 0 0 0-6 2.292m0-14.25v14.25"></path></svg>BibTeX</button></div></div></div></div><div class="bg-white dark:bg-neutral-900 p-6 rounded-xl shadow-sm border border-neutral-200 dark:border-neutral-800 hover:shadow-md transition-all duration-200" style="opacity:0;transform:translateY(20px)"><div class="flex flex-col md:flex-row gap-6"><div class="w-full md:w-48 flex-shrink-0"><div class="aspect-video md:aspect-[4/3] relative rounded-lg overflow-hidden bg-neutral-100 dark:bg-neutral-800"><img alt="IntactKV: Improving Large Language Model Quantization by Keeping Pivot Tokens Intact" loading="lazy" decoding="async" data-nimg="fill" class="object-cover" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;color:transparent" src="/papers/IntactKV.png"/></div></div><div class="flex-grow"><h3 class="text-xl font-semibold text-primary mb-2 leading-tight">IntactKV: Improving Large Language Model Quantization by Keeping Pivot Tokens Intact</h3><p class="text-base text-neutral-600 dark:text-neutral-400 mb-2"><span><span class="">Ruikang Liu</span>, </span><span><span class="">Haoli Bai</span>, </span><span><span class="">Haokun Lin</span>, </span><span><span class="">Yuening Li</span>, </span><span><span class="">Han Gao</span>, </span><span><span class="font-semibold text-accent">Zhengzhuo Xu</span>, </span><span><span class="">Lu Hou</span>, </span><span><span class="">Jun Yao</span>, </span><span><span class="">Chun Yuan</span></span></p><p class="text-sm font-medium text-neutral-800 dark:text-neutral-600 mb-3">Findings of the Association for Computational Linguistics, ACL<!-- --> <!-- -->2024</p><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-4 line-clamp-3">A research on IntactKV framework for large language model quantization, proposing to keep pivot tokens intact to reduce quantization errors and enhance the precision of quantized LLMs in natural language processing tasks.</p><div class="flex flex-wrap gap-2 mt-auto"><a href="https://doi.org/10.18653/V1/2024.FINDINGS-ACL.460" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">DOI</a><button class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium transition-colors bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M12 6.042A8.967 8.967 0 0 0 6 3.75c-1.052 0-2.062.18-3 .512v14.25A8.987 8.987 0 0 1 6 18c2.305 0 4.408.867 6 2.292m0-14.25a8.966 8.966 0 0 1 6-2.292c1.052 0 2.062.18 3 .512v14.25A8.987 8.987 0 0 0 18 18a8.967 8.967 0 0 0-6 2.292m0-14.25v14.25"></path></svg>BibTeX</button></div></div></div></div><div class="bg-white dark:bg-neutral-900 p-6 rounded-xl shadow-sm border border-neutral-200 dark:border-neutral-800 hover:shadow-md transition-all duration-200" style="opacity:0;transform:translateY(20px)"><div class="flex flex-col md:flex-row gap-6"><div class="w-full md:w-48 flex-shrink-0"><div class="aspect-video md:aspect-[4/3] relative rounded-lg overflow-hidden bg-neutral-100 dark:bg-neutral-800"><img alt="Towards Effective Collaborative Learning in Long-Tailed Recognition" loading="lazy" decoding="async" data-nimg="fill" class="object-cover" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;color:transparent" src="/papers/ECL.png"/></div></div><div class="flex-grow"><h3 class="text-xl font-semibold text-primary mb-2 leading-tight">Towards Effective Collaborative Learning in Long-Tailed Recognition</h3><p class="text-base text-neutral-600 dark:text-neutral-400 mb-2"><span><span class="font-semibold text-accent">Zhengzhuo Xu</span>, </span><span><span class="">Zenghao Chai</span>, </span><span><span class="">Chengyin Xu</span>, </span><span><span class="">Chun Yuan</span>, </span><span><span class="">Haiqin Yang</span></span></p><p class="text-sm font-medium text-neutral-800 dark:text-neutral-600 mb-3">IEEE Trans. Multim.<!-- --> <!-- -->2024</p><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-4 line-clamp-3">A research on effective collaborative learning strategies for long-tailed recognition tasks, exploring collaborative mechanisms to address the class imbalance problem in multimedia and computer vision scenarios.</p><div class="flex flex-wrap gap-2 mt-auto"><a href="https://doi.org/10.1109/TMM.2023.3314980" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">DOI</a><button class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium transition-colors bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M12 6.042A8.967 8.967 0 0 0 6 3.75c-1.052 0-2.062.18-3 .512v14.25A8.987 8.987 0 0 1 6 18c2.305 0 4.408.867 6 2.292m0-14.25a8.966 8.966 0 0 1 6-2.292c1.052 0 2.062.18 3 .512v14.25A8.987 8.987 0 0 0 18 18a8.967 8.967 0 0 0-6 2.292m0-14.25v14.25"></path></svg>BibTeX</button></div></div></div></div><div class="bg-white dark:bg-neutral-900 p-6 rounded-xl shadow-sm border border-neutral-200 dark:border-neutral-800 hover:shadow-md transition-all duration-200" style="opacity:0;transform:translateY(20px)"><div class="flex flex-col md:flex-row gap-6"><div class="w-full md:w-48 flex-shrink-0"><div class="aspect-video md:aspect-[4/3] relative rounded-lg overflow-hidden bg-neutral-100 dark:bg-neutral-800"><img alt="ChartBench: A Benchmark for Complex Visual Reasoning in Charts" loading="lazy" decoding="async" data-nimg="fill" class="object-cover" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;color:transparent" src="/papers/ChartBench.png"/></div></div><div class="flex-grow"><h3 class="text-xl font-semibold text-primary mb-2 leading-tight">ChartBench: A Benchmark for Complex Visual Reasoning in Charts</h3><p class="text-base text-neutral-600 dark:text-neutral-400 mb-2"><span><span class="font-semibold text-accent">Zhengzhuo Xu</span>, </span><span><span class="">Sinan Du</span>, </span><span><span class="">Yiyan Qi</span>, </span><span><span class="">Chengjin Xu</span>, </span><span><span class="">Chun Yuan</span>, </span><span><span class="">Jian Guo</span></span></p><p class="text-sm font-medium text-neutral-800 dark:text-neutral-600 mb-3">Arxiv preprint<!-- --> <!-- -->2023</p><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-4 line-clamp-3">A research on ChartBench, a novel benchmark designed to evaluate the complex visual reasoning capabilities of models on chart data, covering diverse chart types and reasoning tasks in visual analytics.</p><div class="flex flex-wrap gap-2 mt-auto"><a href="https://doi.org/10.48550/ARXIV.2312.15915" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">DOI</a><button class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium transition-colors bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M12 6.042A8.967 8.967 0 0 0 6 3.75c-1.052 0-2.062.18-3 .512v14.25A8.987 8.987 0 0 1 6 18c2.305 0 4.408.867 6 2.292m0-14.25a8.966 8.966 0 0 1 6-2.292c1.052 0 2.062.18 3 .512v14.25A8.987 8.987 0 0 0 18 18a8.967 8.967 0 0 0-6 2.292m0-14.25v14.25"></path></svg>BibTeX</button></div></div></div></div><div class="bg-white dark:bg-neutral-900 p-6 rounded-xl shadow-sm border border-neutral-200 dark:border-neutral-800 hover:shadow-md transition-all duration-200" style="opacity:0;transform:translateY(20px)"><div class="flex flex-col md:flex-row gap-6"><div class="w-full md:w-48 flex-shrink-0"><div class="aspect-video md:aspect-[4/3] relative rounded-lg overflow-hidden bg-neutral-100 dark:bg-neutral-800"><img alt="Accurate 3D Face Reconstruction with Facial Component Tokens" loading="lazy" decoding="async" data-nimg="fill" class="object-cover" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;color:transparent" src="/papers/TokenFace.png"/></div></div><div class="flex-grow"><h3 class="text-xl font-semibold text-primary mb-2 leading-tight">Accurate 3D Face Reconstruction with Facial Component Tokens</h3><p class="text-base text-neutral-600 dark:text-neutral-400 mb-2"><span><span class="">Tianke Zhang</span>, </span><span><span class="">Xuangeng Chu</span>, </span><span><span class="">Yunfei Liu</span>, </span><span><span class="">Lijian Lin</span>, </span><span><span class="">Zhendong Yang</span>, </span><span><span class="font-semibold text-accent">Zhengzhuo Xu</span>, </span><span><span class="">Chengkun Cao</span>, </span><span><span class="">Fei Yu</span>, </span><span><span class="">Changyin Zhou</span>, </span><span><span class="">Chun Yuan</span>, </span><span><span class="">Yu Li</span></span></p><p class="text-sm font-medium text-neutral-800 dark:text-neutral-600 mb-3">IEEE/CVF International Conference on Computer Vision, ICCV<!-- --> <!-- -->2023</p><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-4 line-clamp-3">A research on accurate 3D face reconstruction leveraging facial component tokens, proposing a novel token-based approach to enhance the precision and detail of 3D facial structure reconstruction in computer vision tasks.</p><div class="flex flex-wrap gap-2 mt-auto"><a href="https://doi.org/10.1109/ICCV51070.2023.00829" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">DOI</a><button class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium transition-colors bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M12 6.042A8.967 8.967 0 0 0 6 3.75c-1.052 0-2.062.18-3 .512v14.25A8.987 8.987 0 0 1 6 18c2.305 0 4.408.867 6 2.292m0-14.25a8.966 8.966 0 0 1 6-2.292c1.052 0 2.062.18 3 .512v14.25A8.987 8.987 0 0 0 18 18a8.967 8.967 0 0 0-6 2.292m0-14.25v14.25"></path></svg>BibTeX</button></div></div></div></div><div class="bg-white dark:bg-neutral-900 p-6 rounded-xl shadow-sm border border-neutral-200 dark:border-neutral-800 hover:shadow-md transition-all duration-200" style="opacity:0;transform:translateY(20px)"><div class="flex flex-col md:flex-row gap-6"><div class="w-full md:w-48 flex-shrink-0"><div class="aspect-video md:aspect-[4/3] relative rounded-lg overflow-hidden bg-neutral-100 dark:bg-neutral-800"><img alt="Learning Imbalanced Data with Vision Transformers" loading="lazy" decoding="async" data-nimg="fill" class="object-cover" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;color:transparent" src="/papers/LiVT.png"/></div></div><div class="flex-grow"><h3 class="text-xl font-semibold text-primary mb-2 leading-tight">Learning Imbalanced Data with Vision Transformers</h3><p class="text-base text-neutral-600 dark:text-neutral-400 mb-2"><span><span class="font-semibold text-accent">Zhengzhuo Xu</span>, </span><span><span class="">Ruikang Liu</span>, </span><span><span class="">Shuo Yang</span>, </span><span><span class="">Zenghao Chai</span>, </span><span><span class="">Chun Yuan</span></span></p><p class="text-sm font-medium text-neutral-800 dark:text-neutral-600 mb-3">IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR<!-- --> <!-- -->2023</p><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-4 line-clamp-3">A research on learning imbalanced data with Vision Transformer models, proposing novel strategies to address the class imbalance challenge in computer vision tasks using ViT architectures.</p><div class="flex flex-wrap gap-2 mt-auto"><a href="https://doi.org/10.1109/CVPR52729.2023.01516" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">DOI</a><button class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium transition-colors bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M12 6.042A8.967 8.967 0 0 0 6 3.75c-1.052 0-2.062.18-3 .512v14.25A8.987 8.987 0 0 1 6 18c2.305 0 4.408.867 6 2.292m0-14.25a8.966 8.966 0 0 1 6-2.292c1.052 0 2.062.18 3 .512v14.25A8.987 8.987 0 0 0 18 18a8.967 8.967 0 0 0-6 2.292m0-14.25v14.25"></path></svg>BibTeX</button></div></div></div></div><div class="bg-white dark:bg-neutral-900 p-6 rounded-xl shadow-sm border border-neutral-200 dark:border-neutral-800 hover:shadow-md transition-all duration-200" style="opacity:0;transform:translateY(20px)"><div class="flex flex-col md:flex-row gap-6"><div class="w-full md:w-48 flex-shrink-0"><div class="aspect-video md:aspect-[4/3] relative rounded-lg overflow-hidden bg-neutral-100 dark:bg-neutral-800"><img alt="Rethink Long-Tailed Recognition with Vision Transforms" loading="lazy" decoding="async" data-nimg="fill" class="object-cover" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;color:transparent" src="/papers/RethinkLT.png"/></div></div><div class="flex-grow"><h3 class="text-xl font-semibold text-primary mb-2 leading-tight">Rethink Long-Tailed Recognition with Vision Transforms</h3><p class="text-base text-neutral-600 dark:text-neutral-400 mb-2"><span><span class="font-semibold text-accent">Zhengzhuo Xu</span>, </span><span><span class="">Shuo Yang</span>, </span><span><span class="">Xingjun Wang</span>, </span><span><span class="">Chun Yuan</span></span></p><p class="text-sm font-medium text-neutral-800 dark:text-neutral-600 mb-3">International Conference on Acoustics, Speech and Signal Processing ICASSP<!-- --> <!-- -->2023</p><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-4 line-clamp-3">A research rethinking long-tailed recognition tasks with Vision Transformer models, exploring the advantages of ViT in addressing the class imbalance problem in long-tailed visual recognition scenarios.</p><div class="flex flex-wrap gap-2 mt-auto"><a href="https://doi.org/10.1109/ICASSP49357.2023.10096734" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">DOI</a><button class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium transition-colors bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M12 6.042A8.967 8.967 0 0 0 6 3.75c-1.052 0-2.062.18-3 .512v14.25A8.987 8.987 0 0 1 6 18c2.305 0 4.408.867 6 2.292m0-14.25a8.966 8.966 0 0 1 6-2.292c1.052 0 2.062.18 3 .512v14.25A8.987 8.987 0 0 0 18 18a8.967 8.967 0 0 0-6 2.292m0-14.25v14.25"></path></svg>BibTeX</button></div></div></div></div><div class="bg-white dark:bg-neutral-900 p-6 rounded-xl shadow-sm border border-neutral-200 dark:border-neutral-800 hover:shadow-md transition-all duration-200" style="opacity:0;transform:translateY(20px)"><div class="flex flex-col md:flex-row gap-6"><div class="w-full md:w-48 flex-shrink-0"><div class="aspect-video md:aspect-[4/3] relative rounded-lg overflow-hidden bg-neutral-100 dark:bg-neutral-800"><img alt="A Lightweight Approach for Network Intrusion Detection Based on Self-Knowledge Distillation" loading="lazy" decoding="async" data-nimg="fill" class="object-cover" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;color:transparent" src="/papers/LNet.png"/></div></div><div class="flex-grow"><h3 class="text-xl font-semibold text-primary mb-2 leading-tight">A Lightweight Approach for Network Intrusion Detection Based on Self-Knowledge Distillation</h3><p class="text-base text-neutral-600 dark:text-neutral-400 mb-2"><span><span class="">Shuo Yang</span>, </span><span><span class="">Xinran Zheng</span>, </span><span><span class="font-semibold text-accent">Zhengzhuo Xu</span>, </span><span><span class="">Xingjun Wang</span></span></p><p class="text-sm font-medium text-neutral-800 dark:text-neutral-600 mb-3">IEEE International Conference on Communications, ICC<!-- --> <!-- -->2023</p><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-4 line-clamp-3">A research on a lightweight network intrusion detection approach based on self-knowledge distillation, designed to enhance the efficiency and accuracy of intrusion detection in communication networks.</p><div class="flex flex-wrap gap-2 mt-auto"><a href="https://doi.org/10.1109/ICC45041.2023.10279691" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">DOI</a><button class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium transition-colors bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M12 6.042A8.967 8.967 0 0 0 6 3.75c-1.052 0-2.062.18-3 .512v14.25A8.987 8.987 0 0 1 6 18c2.305 0 4.408.867 6 2.292m0-14.25a8.966 8.966 0 0 1 6-2.292c1.052 0 2.062.18 3 .512v14.25A8.987 8.987 0 0 0 18 18a8.967 8.967 0 0 0-6 2.292m0-14.25v14.25"></path></svg>BibTeX</button></div></div></div></div><div class="bg-white dark:bg-neutral-900 p-6 rounded-xl shadow-sm border border-neutral-200 dark:border-neutral-800 hover:shadow-md transition-all duration-200" style="opacity:0;transform:translateY(20px)"><div class="flex flex-col md:flex-row gap-6"><div class="w-full md:w-48 flex-shrink-0"><div class="aspect-video md:aspect-[4/3] relative rounded-lg overflow-hidden bg-neutral-100 dark:bg-neutral-800"><img alt="HHF: Hashing-Guided Hinge Function for Deep Hashing Retrieval" loading="lazy" decoding="async" data-nimg="fill" class="object-cover" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;color:transparent" src="/papers/HHF.png"/></div></div><div class="flex-grow"><h3 class="text-xl font-semibold text-primary mb-2 leading-tight">HHF: Hashing-Guided Hinge Function for Deep Hashing Retrieval</h3><p class="text-base text-neutral-600 dark:text-neutral-400 mb-2"><span><span class="">Chengyin Xu</span>, </span><span><span class="">Zenghao Chai</span>, </span><span><span class="font-semibold text-accent">Zhengzhuo Xu</span>, </span><span><span class="">Hongjia Li</span>, </span><span><span class="">Qiruyi Zuo</span>, </span><span><span class="">Lingyu Yang</span>, </span><span><span class="">Chun Yuan</span></span></p><p class="text-sm font-medium text-neutral-800 dark:text-neutral-600 mb-3">IEEE Trans. Multim.<!-- --> <!-- -->2023</p><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-4 line-clamp-3">A research on the HHF (Hashing-Guided Hinge Function) framework for deep hashing retrieval, leveraging hashing guidance to optimize hinge functions and enhance the efficiency and accuracy of deep hashing-based retrieval tasks.</p><div class="flex flex-wrap gap-2 mt-auto"><a href="https://doi.org/10.1109/TMM.2022.3222598" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">DOI</a><button class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium transition-colors bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M12 6.042A8.967 8.967 0 0 0 6 3.75c-1.052 0-2.062.18-3 .512v14.25A8.987 8.987 0 0 1 6 18c2.305 0 4.408.867 6 2.292m0-14.25a8.966 8.966 0 0 1 6-2.292c1.052 0 2.062.18 3 .512v14.25A8.987 8.987 0 0 0 18 18a8.967 8.967 0 0 0-6 2.292m0-14.25v14.25"></path></svg>BibTeX</button></div></div></div></div><div class="bg-white dark:bg-neutral-900 p-6 rounded-xl shadow-sm border border-neutral-200 dark:border-neutral-800 hover:shadow-md transition-all duration-200" style="opacity:0;transform:translateY(20px)"><div class="flex flex-col md:flex-row gap-6"><div class="w-full md:w-48 flex-shrink-0"><div class="aspect-video md:aspect-[4/3] relative rounded-lg overflow-hidden bg-neutral-100 dark:bg-neutral-800"><img alt="HyP^2 Loss: Beyond Hypersphere Metric Space for Multi-label Image Retrieval" loading="lazy" decoding="async" data-nimg="fill" class="object-cover" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;color:transparent" src="/papers/HpyLoss.png"/></div></div><div class="flex-grow"><h3 class="text-xl font-semibold text-primary mb-2 leading-tight">HyP^2 Loss: Beyond Hypersphere Metric Space for Multi-label Image Retrieval</h3><p class="text-base text-neutral-600 dark:text-neutral-400 mb-2"><span><span class="">Chengyin Xu</span>, </span><span><span class="">Zenghao Chai</span>, </span><span><span class="font-semibold text-accent">Zhengzhuo Xu</span>, </span><span><span class="">Chun Yuan</span>, </span><span><span class="">Yanbo Fan</span>, </span><span><span class="">Jue Wang</span></span></p><p class="text-sm font-medium text-neutral-800 dark:text-neutral-600 mb-3">ACM International Conference on Multimedia, ACM MM<!-- --> <!-- -->2022</p><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-4 line-clamp-3">A research on the HyP(^mbox2) Loss function that breaks through the limitations of the hypersphere metric space, aiming to improve the performance of multi-label image retrieval tasks in multimedia processing.</p><div class="flex flex-wrap gap-2 mt-auto"><a href="https://doi.org/10.1145/3503161.3548032" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">DOI</a><button class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium transition-colors bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M12 6.042A8.967 8.967 0 0 0 6 3.75c-1.052 0-2.062.18-3 .512v14.25A8.987 8.987 0 0 1 6 18c2.305 0 4.408.867 6 2.292m0-14.25a8.966 8.966 0 0 1 6-2.292c1.052 0 2.062.18 3 .512v14.25A8.987 8.987 0 0 0 18 18a8.967 8.967 0 0 0-6 2.292m0-14.25v14.25"></path></svg>BibTeX</button></div></div></div></div><div class="bg-white dark:bg-neutral-900 p-6 rounded-xl shadow-sm border border-neutral-200 dark:border-neutral-800 hover:shadow-md transition-all duration-200" style="opacity:0;transform:translateY(20px)"><div class="flex flex-col md:flex-row gap-6"><div class="w-full md:w-48 flex-shrink-0"><div class="aspect-video md:aspect-[4/3] relative rounded-lg overflow-hidden bg-neutral-100 dark:bg-neutral-800"><img alt="REALY: Rethinking the Evaluation of 3D Face Reconstruction" loading="lazy" decoding="async" data-nimg="fill" class="object-cover" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;color:transparent" src="/papers/REALY.png"/></div></div><div class="flex-grow"><h3 class="text-xl font-semibold text-primary mb-2 leading-tight">REALY: Rethinking the Evaluation of 3D Face Reconstruction</h3><p class="text-base text-neutral-600 dark:text-neutral-400 mb-2"><span><span class="">Zenghao Chai</span>, </span><span><span class="">Haoxian Zhang</span>, </span><span><span class="">Jing Ren</span>, </span><span><span class="">Di Kang</span>, </span><span><span class="font-semibold text-accent">Zhengzhuo Xu</span>, </span><span><span class="">Xuefei Zhe</span>, </span><span><span class="">Chun Yuan</span>, </span><span><span class="">Linchao Bao</span></span></p><p class="text-sm font-medium text-neutral-800 dark:text-neutral-600 mb-3">European Conference Computer Vision, ECCV<!-- --> <!-- -->2022</p><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-4 line-clamp-3">A research on REALY framework that rethinks the evaluation metrics and methodologies for 3D face reconstruction, aiming to provide more accurate and comprehensive evaluation criteria for the field.</p><div class="flex flex-wrap gap-2 mt-auto"><a href="https://doi.org/10.1007/978-3-031-20074-8_5" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">DOI</a><button class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium transition-colors bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M12 6.042A8.967 8.967 0 0 0 6 3.75c-1.052 0-2.062.18-3 .512v14.25A8.987 8.987 0 0 1 6 18c2.305 0 4.408.867 6 2.292m0-14.25a8.966 8.966 0 0 1 6-2.292c1.052 0 2.062.18 3 .512v14.25A8.987 8.987 0 0 0 18 18a8.967 8.967 0 0 0-6 2.292m0-14.25v14.25"></path></svg>BibTeX</button></div></div></div></div><div class="bg-white dark:bg-neutral-900 p-6 rounded-xl shadow-sm border border-neutral-200 dark:border-neutral-800 hover:shadow-md transition-all duration-200" style="opacity:0;transform:translateY(20px)"><div class="flex flex-col md:flex-row gap-6"><div class="w-full md:w-48 flex-shrink-0"><div class="aspect-video md:aspect-[4/3] relative rounded-lg overflow-hidden bg-neutral-100 dark:bg-neutral-800"><img alt="Semantic-Sparse Colorization Network for Deep Exemplar-Based Colorization" loading="lazy" decoding="async" data-nimg="fill" class="object-cover" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;color:transparent" src="/papers/ColorNet.png"/></div></div><div class="flex-grow"><h3 class="text-xl font-semibold text-primary mb-2 leading-tight">Semantic-Sparse Colorization Network for Deep Exemplar-Based Colorization</h3><p class="text-base text-neutral-600 dark:text-neutral-400 mb-2"><span><span class="">Yunpeng Bai</span>, </span><span><span class="">Chao Dong</span>, </span><span><span class="">Zenghao Chai</span>, </span><span><span class="">Andong Wang</span>, </span><span><span class="font-semibold text-accent">Zhengzhuo Xu</span>, </span><span><span class="">Chun Yuan</span></span></p><p class="text-sm font-medium text-neutral-800 dark:text-neutral-600 mb-3">European Conference Computer Vision, ECCV<!-- --> <!-- -->2022</p><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-4 line-clamp-3">A research on the Semantic-Sparse Colorization Network, proposing a novel deep exemplar-based colorization approach that leverages semantic sparsity to enhance colorization accuracy and realism.</p><div class="flex flex-wrap gap-2 mt-auto"><a href="https://doi.org/10.1007/978-3-031-20068-7_29" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">DOI</a><button class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium transition-colors bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M12 6.042A8.967 8.967 0 0 0 6 3.75c-1.052 0-2.062.18-3 .512v14.25A8.987 8.987 0 0 1 6 18c2.305 0 4.408.867 6 2.292m0-14.25a8.966 8.966 0 0 1 6-2.292c1.052 0 2.062.18 3 .512v14.25A8.987 8.987 0 0 0 18 18a8.967 8.967 0 0 0-6 2.292m0-14.25v14.25"></path></svg>BibTeX</button></div></div></div></div><div class="bg-white dark:bg-neutral-900 p-6 rounded-xl shadow-sm border border-neutral-200 dark:border-neutral-800 hover:shadow-md transition-all duration-200" style="opacity:0;transform:translateY(20px)"><div class="flex flex-col md:flex-row gap-6"><div class="w-full md:w-48 flex-shrink-0"><div class="aspect-video md:aspect-[4/3] relative rounded-lg overflow-hidden bg-neutral-100 dark:bg-neutral-800"><img alt="CMS-LSTM: Context Embedding and Multi-Scale Spatiotemporal Expression LSTM for Predictive Learning" loading="lazy" decoding="async" data-nimg="fill" class="object-cover" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;color:transparent" src="/papers/ICME2022.png"/></div></div><div class="flex-grow"><h3 class="text-xl font-semibold text-primary mb-2 leading-tight">CMS-LSTM: Context Embedding and Multi-Scale Spatiotemporal Expression LSTM for Predictive Learning</h3><p class="text-base text-neutral-600 dark:text-neutral-400 mb-2"><span><span class="">Zenghao Chai</span>, </span><span><span class="font-semibold text-accent">Zhengzhuo Xu</span>, </span><span><span class="">Yunpeng Bai</span>, </span><span><span class="">Zhihui Lin</span>, </span><span><span class="">Chun Yuan</span></span></p><p class="text-sm font-medium text-neutral-800 dark:text-neutral-600 mb-3">IEEE International Conference on Multimedia and Expo, ICME<!-- --> <!-- -->2022</p><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-4 line-clamp-3">A study on the CMS-LSTM framework integrating context embedding and multi-scale spatiotemporal expression, designed to boost predictive learning performance in multimedia spatiotemporal tasks.</p><div class="flex flex-wrap gap-2 mt-auto"><a href="https://doi.org/10.1109/ICME52920.2022.9859659" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">DOI</a><button class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium transition-colors bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M12 6.042A8.967 8.967 0 0 0 6 3.75c-1.052 0-2.062.18-3 .512v14.25A8.987 8.987 0 0 1 6 18c2.305 0 4.408.867 6 2.292m0-14.25a8.966 8.966 0 0 1 6-2.292c1.052 0 2.062.18 3 .512v14.25A8.987 8.987 0 0 0 18 18a8.967 8.967 0 0 0-6 2.292m0-14.25v14.25"></path></svg>BibTeX</button></div></div></div></div><div class="bg-white dark:bg-neutral-900 p-6 rounded-xl shadow-sm border border-neutral-200 dark:border-neutral-800 hover:shadow-md transition-all duration-200" style="opacity:0;transform:translateY(20px)"><div class="flex flex-col md:flex-row gap-6"><div class="w-full md:w-48 flex-shrink-0"><div class="aspect-video md:aspect-[4/3] relative rounded-lg overflow-hidden bg-neutral-100 dark:bg-neutral-800"><img alt="Modernn: Towards Fine-Grained Motion Details for Spatiotemporal Predictive Learning" loading="lazy" decoding="async" data-nimg="fill" class="object-cover" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;color:transparent" src="/papers/ICASSP2022.png"/></div></div><div class="flex-grow"><h3 class="text-xl font-semibold text-primary mb-2 leading-tight">Modernn: Towards Fine-Grained Motion Details for Spatiotemporal Predictive Learning</h3><p class="text-base text-neutral-600 dark:text-neutral-400 mb-2"><span><span class="">Zenghao Chai</span>, </span><span><span class="font-semibold text-accent">Zhengzhuo Xu</span>, </span><span><span class="">Chun Yuan</span></span></p><p class="text-sm font-medium text-neutral-800 dark:text-neutral-600 mb-3">IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP<!-- --> <!-- -->2022</p><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-4 line-clamp-3">A research on Modernn framework for capturing fine-grained motion details, aiming to enhance spatiotemporal predictive learning performance in visual signal processing tasks.</p><div class="flex flex-wrap gap-2 mt-auto"><a href="https://doi.org/10.1109/ICASSP43922.2022.9747035" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">DOI</a><button class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium transition-colors bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M12 6.042A8.967 8.967 0 0 0 6 3.75c-1.052 0-2.062.18-3 .512v14.25A8.987 8.987 0 0 1 6 18c2.305 0 4.408.867 6 2.292m0-14.25a8.966 8.966 0 0 1 6-2.292c1.052 0 2.062.18 3 .512v14.25A8.987 8.987 0 0 0 18 18a8.967 8.967 0 0 0-6 2.292m0-14.25v14.25"></path></svg>BibTeX</button></div></div></div></div><div class="bg-white dark:bg-neutral-900 p-6 rounded-xl shadow-sm border border-neutral-200 dark:border-neutral-800 hover:shadow-md transition-all duration-200" style="opacity:0;transform:translateY(20px)"><div class="flex flex-col md:flex-row gap-6"><div class="w-full md:w-48 flex-shrink-0"><div class="aspect-video md:aspect-[4/3] relative rounded-lg overflow-hidden bg-neutral-100 dark:bg-neutral-800"><img alt="Towards Calibrated Model for Long-Tailed Visual Recognition from Prior Perspective" loading="lazy" decoding="async" data-nimg="fill" class="object-cover" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;color:transparent" src="/papers/NIPS2021.png"/></div></div><div class="flex-grow"><h3 class="text-xl font-semibold text-primary mb-2 leading-tight">Towards Calibrated Model for Long-Tailed Visual Recognition from Prior Perspective</h3><p class="text-base text-neutral-600 dark:text-neutral-400 mb-2"><span><span class="font-semibold text-accent">Zhengzhuo Xu</span>, </span><span><span class="">Zenghao Chai</span>, </span><span><span class="">Chun Yuan</span></span></p><p class="text-sm font-medium text-neutral-800 dark:text-neutral-600 mb-3">Advances in Neural Information Processing Systems, NeurIPS<!-- --> <!-- -->2021</p><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-4 line-clamp-3">A research on calibrated models for long-tailed visual recognition from a prior perspective, addressing the calibration and classification challenges in long-tailed visual data scenarios.</p><div class="flex flex-wrap gap-2 mt-auto"><a href="https://doi.org/10.48550/arXiv.2110.04456" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">DOI</a><button class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium transition-colors bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M12 6.042A8.967 8.967 0 0 0 6 3.75c-1.052 0-2.062.18-3 .512v14.25A8.987 8.987 0 0 1 6 18c2.305 0 4.408.867 6 2.292m0-14.25a8.966 8.966 0 0 1 6-2.292c1.052 0 2.062.18 3 .512v14.25A8.987 8.987 0 0 0 18 18a8.967 8.967 0 0 0-6 2.292m0-14.25v14.25"></path></svg>BibTeX</button></div></div></div></div><div class="bg-white dark:bg-neutral-900 p-6 rounded-xl shadow-sm border border-neutral-200 dark:border-neutral-800 hover:shadow-md transition-all duration-200" style="opacity:0;transform:translateY(20px)"><div class="flex flex-col md:flex-row gap-6"><div class="w-full md:w-48 flex-shrink-0"><div class="aspect-video md:aspect-[4/3] relative rounded-lg overflow-hidden bg-neutral-100 dark:bg-neutral-800"><img alt="Backscatter-Assisted Hybrid Relaying Strategy for Wireless Powered IoT Communications" loading="lazy" decoding="async" data-nimg="fill" class="object-cover" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;color:transparent" src="/papers/GLOBECOM.png"/></div></div><div class="flex-grow"><h3 class="text-xl font-semibold text-primary mb-2 leading-tight">Backscatter-Assisted Hybrid Relaying Strategy for Wireless Powered IoT Communications</h3><p class="text-base text-neutral-600 dark:text-neutral-400 mb-2"><span><span class="">Yutong Xie</span>, </span><span><span class="font-semibold text-accent">Zhengzhuo Xu</span>, </span><span><span class="">Shimin Gong</span>, </span><span><span class="">Jing Xu</span>, </span><span><span class="">Dinh Thai Hoang</span>, </span><span><span class="">Dusit Niyato</span></span></p><p class="text-sm font-medium text-neutral-800 dark:text-neutral-600 mb-3">IEEE Global Communications Conference, GLOBECOM<!-- --> <!-- -->2019</p><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-4 line-clamp-3">A study on backscatter-assisted hybrid relaying strategies for wireless powered IoT communications, exploring optimized relaying schemes to enhance the performance of IoT communication systems.</p><div class="flex flex-wrap gap-2 mt-auto"><a href="https://doi.org/10.1109/GLOBECOM38437.2019.9013386" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">DOI</a><button class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium transition-colors bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M12 6.042A8.967 8.967 0 0 0 6 3.75c-1.052 0-2.062.18-3 .512v14.25A8.987 8.987 0 0 1 6 18c2.305 0 4.408.867 6 2.292m0-14.25a8.966 8.966 0 0 1 6-2.292c1.052 0 2.062.18 3 .512v14.25A8.987 8.987 0 0 0 18 18a8.967 8.967 0 0 0-6 2.292m0-14.25v14.25"></path></svg>BibTeX</button></div></div></div></div><div class="bg-white dark:bg-neutral-900 p-6 rounded-xl shadow-sm border border-neutral-200 dark:border-neutral-800 hover:shadow-md transition-all duration-200" style="opacity:0;transform:translateY(20px)"><div class="flex flex-col md:flex-row gap-6"><div class="w-full md:w-48 flex-shrink-0"><div class="aspect-video md:aspect-[4/3] relative rounded-lg overflow-hidden bg-neutral-100 dark:bg-neutral-800"><img alt="Backscatter-Assisted Computation Offloading for Energy Harvesting IoT Devices via Policy-based Deep Reinforcement Learning" loading="lazy" decoding="async" data-nimg="fill" class="object-cover" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;color:transparent" src="/papers/ICCC.png"/></div></div><div class="flex-grow"><h3 class="text-xl font-semibold text-primary mb-2 leading-tight">Backscatter-Assisted Computation Offloading for Energy Harvesting IoT Devices via Policy-based Deep Reinforcement Learning</h3><p class="text-base text-neutral-600 dark:text-neutral-400 mb-2"><span><span class="">Yutong Xie</span>, </span><span><span class="font-semibold text-accent">Zhengzhuo Xu</span>, </span><span><span class="">Yuxing Zhong</span>, </span><span><span class="">Jing Xu</span>, </span><span><span class="">Shimin Gong</span>, </span><span><span class="">Yi Wang</span></span></p><p class="text-sm font-medium text-neutral-800 dark:text-neutral-600 mb-3">IEEE/CIC International Conference on Communications Workshops<!-- --> <!-- -->2019</p><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-4 line-clamp-3">A research on backscatter-assisted computation offloading for energy harvesting IoT devices, adopting policy-based deep reinforcement learning to optimize offloading decisions for energy-constrained IoT scenarios.</p><div class="flex flex-wrap gap-2 mt-auto"><a href="https://doi.org/10.1109/ICCChinaW.2019.8849964" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">DOI</a><button class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium transition-colors bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M12 6.042A8.967 8.967 0 0 0 6 3.75c-1.052 0-2.062.18-3 .512v14.25A8.987 8.987 0 0 1 6 18c2.305 0 4.408.867 6 2.292m0-14.25a8.966 8.966 0 0 1 6-2.292c1.052 0 2.062.18 3 .512v14.25A8.987 8.987 0 0 0 18 18a8.967 8.967 0 0 0-6 2.292m0-14.25v14.25"></path></svg>BibTeX</button></div></div></div></div><div class="bg-white dark:bg-neutral-900 p-6 rounded-xl shadow-sm border border-neutral-200 dark:border-neutral-800 hover:shadow-md transition-all duration-200" style="opacity:0;transform:translateY(20px)"><div class="flex flex-col md:flex-row gap-6"><div class="w-full md:w-48 flex-shrink-0"><div class="aspect-video md:aspect-[4/3] relative rounded-lg overflow-hidden bg-neutral-100 dark:bg-neutral-800"><img alt="Backscatter-Aided Hybrid Data Offloading for Mobile Edge Computing via Deep Reinforcement Learning" loading="lazy" decoding="async" data-nimg="fill" class="object-cover" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;color:transparent" src="/papers/MLICOM.png"/></div></div><div class="flex-grow"><h3 class="text-xl font-semibold text-primary mb-2 leading-tight">Backscatter-Aided Hybrid Data Offloading for Mobile Edge Computing via Deep Reinforcement Learning</h3><p class="text-base text-neutral-600 dark:text-neutral-400 mb-2"><span><span class="">Yutong Xie</span>, </span><span><span class="font-semibold text-accent">Zhengzhuo Xu</span>, </span><span><span class="">Jing Xu</span>, </span><span><span class="">Shimin Gong</span>, </span><span><span class="">Yi Wang</span></span></p><p class="text-sm font-medium text-neutral-800 dark:text-neutral-600 mb-3">Machine Learning and Intelligent Communications, MLICOM<!-- --> <!-- -->2019</p><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-4 line-clamp-3">A study on backscatter-aided hybrid data offloading for mobile edge computing, leveraging deep reinforcement learning to optimize the offloading strategy.</p><div class="flex flex-wrap gap-2 mt-auto"><a href="https://doi.org/10.1007/978-3-030-32388-2_45" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">DOI</a><button class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium transition-colors bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M12 6.042A8.967 8.967 0 0 0 6 3.75c-1.052 0-2.062.18-3 .512v14.25A8.987 8.987 0 0 1 6 18c2.305 0 4.408.867 6 2.292m0-14.25a8.966 8.966 0 0 1 6-2.292c1.052 0 2.062.18 3 .512v14.25A8.987 8.987 0 0 0 18 18a8.967 8.967 0 0 0-6 2.292m0-14.25v14.25"></path></svg>BibTeX</button></div></div></div></div></div></div></div><!--$--><!--/$--><!--$--><!--/$--></main><footer class="border-t border-neutral-200/50 bg-neutral-50/50 dark:bg-neutral-900/50 dark:border-neutral-700/50"><div class="max-w-6xl mx-auto px-4 sm:px-6 lg:px-8 py-6"><div class="flex flex-col sm:flex-row justify-between items-center gap-2"><p class="text-xs text-neutral-500">Last updated: <!-- -->November 30, 2025</p><p class="text-xs text-neutral-500 flex items-center"><a href="https://github.com/xyjoey/PRISM" target="_blank" rel="noopener noreferrer">Built with PRISM</a><span class="ml-2">🚀</span></p></div></div></footer></div><script src="/_next/static/chunks/webpack-679b75d1c4c2027c.js" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0])</script><script>self.__next_f.push([1,"1:\"$Sreact.fragment\"\n2:I[3719,[\"17\",\"static/chunks/17-9c04c9413a9f9f1f.js\",\"874\",\"static/chunks/874-6cc630662f3664af.js\",\"862\",\"static/chunks/862-15038af301665bb3.js\",\"177\",\"static/chunks/app/layout-347869ba10f90412.js\"],\"ThemeProvider\"]\n3:I[768,[\"17\",\"static/chunks/17-9c04c9413a9f9f1f.js\",\"874\",\"static/chunks/874-6cc630662f3664af.js\",\"862\",\"static/chunks/862-15038af301665bb3.js\",\"177\",\"static/chunks/app/layout-347869ba10f90412.js\"],\"default\"]\n4:I[7555,[],\"\"]\n5:I[1295,[],\"\"]\n6:I[167,[\"17\",\"static/chunks/17-9c04c9413a9f9f1f.js\",\"874\",\"static/chunks/874-6cc630662f3664af.js\",\"862\",\"static/chunks/862-15038af301665bb3.js\",\"177\",\"static/chunks/app/layout-347869ba10f90412.js\"],\"default\"]\n8:I[9665,[],\"MetadataBoundary\"]\na:I[9665,[],\"OutletBoundary\"]\nd:I[4911,[],\"AsyncMetadataOutlet\"]\nf:I[9665,[],\"ViewportBoundary\"]\n11:I[6614,[],\"\"]\n:HL[\"/_next/static/css/19bf7477d82f620e.css\",\"style\"]\n"])</script><script>self.__next_f.push([1,"0:{\"P\":null,\"b\":\"cEsdeqwoI7w1tSIFhRUgz\",\"p\":\"\",\"c\":[\"\",\"publications\",\"\"],\"i\":false,\"f\":[[[\"\",{\"children\":[[\"slug\",\"publications\",\"d\"],{\"children\":[\"__PAGE__\",{}]}]},\"$undefined\",\"$undefined\",true],[\"\",[\"$\",\"$1\",\"c\",{\"children\":[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/19bf7477d82f620e.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}]],[\"$\",\"html\",null,{\"lang\":\"en\",\"className\":\"scroll-smooth\",\"suppressHydrationWarning\":true,\"children\":[[\"$\",\"head\",null,{\"children\":[[\"$\",\"link\",null,{\"rel\":\"icon\",\"href\":\"/favicon.svg\",\"type\":\"image/svg+xml\"}],[\"$\",\"link\",null,{\"rel\":\"dns-prefetch\",\"href\":\"https://google-fonts.jialeliu.com\"}],[\"$\",\"link\",null,{\"rel\":\"preconnect\",\"href\":\"https://google-fonts.jialeliu.com\",\"crossOrigin\":\"\"}],[\"$\",\"link\",null,{\"rel\":\"preload\",\"as\":\"style\",\"href\":\"https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700\u0026family=Crimson+Text:ital,wght@0,400;0,600;1,400\u0026display=swap\"}],[\"$\",\"link\",null,{\"rel\":\"stylesheet\",\"id\":\"gfonts-css\",\"href\":\"https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700\u0026family=Crimson+Text:ital,wght@0,400;0,600;1,400\u0026display=swap\",\"media\":\"print\"}],[\"$\",\"script\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"\\n              (function(){\\n                var l = document.getElementById('gfonts-css');\\n                if (!l) return;\\n                if (l.media !== 'all') {\\n                  l.addEventListener('load', function(){ try { l.media = 'all'; } catch(e){} });\\n                }\\n              })();\\n            \"}}],[\"$\",\"noscript\",null,{\"children\":[\"$\",\"link\",null,{\"rel\":\"stylesheet\",\"href\":\"https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700\u0026family=Crimson+Text:ital,wght@0,400;0,600;1,400\u0026display=swap\"}]}],[\"$\",\"script\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"\\n              try {\\n                const theme = localStorage.getItem('theme-storage');\\n                const parsed = theme ? JSON.parse(theme) : null;\\n                const setting = parsed?.state?.theme || 'system';\\n                const prefersDark = typeof window !== 'undefined' \u0026\u0026 window.matchMedia \u0026\u0026 window.matchMedia('(prefers-color-scheme: dark)').matches;\\n                const effective = setting === 'dark' ? 'dark' : (setting === 'light' ? 'light' : (prefersDark ? 'dark' : 'light'));\\n                var root = document.documentElement;\\n                root.classList.add(effective);\\n                root.setAttribute('data-theme', effective);\\n              } catch (e) {\\n                var root = document.documentElement;\\n                root.classList.add('light');\\n                root.setAttribute('data-theme', 'light');\\n              }\\n            \"}}]]}],[\"$\",\"body\",null,{\"className\":\"font-sans antialiased\",\"children\":[\"$\",\"$L2\",null,{\"children\":[[\"$\",\"$L3\",null,{\"items\":[{\"title\":\"About\",\"type\":\"page\",\"target\":\"about\",\"href\":\"/\"},{\"title\":\"Education\",\"type\":\"page\",\"target\":\"education\",\"href\":\"/education\"},{\"title\":\"Publications\",\"type\":\"page\",\"target\":\"publications\",\"href\":\"/publications\"},{\"title\":\"Career\",\"type\":\"page\",\"target\":\"career\",\"href\":\"/career\"},{\"title\":\"Awards\",\"type\":\"page\",\"target\":\"awards\",\"href\":\"/awards\"}],\"siteTitle\":\"Zhengzhuo Xu (许正卓)\",\"enableOnePageMode\":false}],[\"$\",\"main\",null,{\"className\":\"min-h-screen pt-16 lg:pt-20\",\"children\":[\"$\",\"$L4\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L5\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[[[\"$\",\"title\",null,{\"children\":\"404: This page could not be found.\"}],[\"$\",\"div\",null,{\"style\":{\"fontFamily\":\"system-ui,\\\"Segoe UI\\\",Roboto,Helvetica,Arial,sans-serif,\\\"Apple Color Emoji\\\",\\\"Segoe UI Emoji\\\"\",\"height\":\"100vh\",\"textAlign\":\"center\",\"display\":\"flex\",\"flexDirection\":\"column\",\"alignItems\":\"center\",\"justifyContent\":\"center\"},\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"style\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\"}}],[\"$\",\"h1\",null,{\"className\":\"next-error-h1\",\"style\":{\"display\":\"inline-block\",\"margin\":\"0 20px 0 0\",\"padding\":\"0 23px 0 0\",\"fontSize\":24,\"fontWeight\":500,\"verticalAlign\":\"top\",\"lineHeight\":\"49px\"},\"children\":404}],[\"$\",\"div\",null,{\"style\":{\"display\":\"inline-block\"},\"children\":[\"$\",\"h2\",null,{\"style\":{\"fontSize\":14,\"fontWeight\":400,\"lineHeight\":\"49px\",\"margin\":0},\"children\":\"This page could not be found.\"}]}]]}]}]],[]],\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]}],[\"$\",\"$L6\",null,{\"lastUpdated\":\"November 30, 2025\"}]]}]}]]}]]}],{\"children\":[[\"slug\",\"publications\",\"d\"],[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L4\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L5\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[\"__PAGE__\",[\"$\",\"$1\",\"c\",{\"children\":[\"$L7\",[\"$\",\"$L8\",null,{\"children\":\"$L9\"}],null,[\"$\",\"$La\",null,{\"children\":[\"$Lb\",\"$Lc\",[\"$\",\"$Ld\",null,{\"promise\":\"$@e\"}]]}]]}],{},null,false]},null,false]},null,false],[\"$\",\"$1\",\"h\",{\"children\":[null,[\"$\",\"$1\",\"SdckpMTe7nWWcnYey9zIR\",{\"children\":[[\"$\",\"$Lf\",null,{\"children\":\"$L10\"}],null]}],null]}],false]],\"m\":\"$undefined\",\"G\":[\"$11\",\"$undefined\"],\"s\":false,\"S\":true}\n"])</script><script>self.__next_f.push([1,"12:\"$Sreact.suspense\"\n13:I[4911,[],\"AsyncMetadata\"]\n15:I[6669,[\"17\",\"static/chunks/17-9c04c9413a9f9f1f.js\",\"178\",\"static/chunks/178-595a94b9af1e67b5.js\",\"748\",\"static/chunks/748-1f3129a1e6365cf9.js\",\"182\",\"static/chunks/app/%5Bslug%5D/page-78388292e0713c46.js\"],\"default\"]\n9:[\"$\",\"$12\",null,{\"fallback\":null,\"children\":[\"$\",\"$L13\",null,{\"promise\":\"$@14\"}]}]\n"])</script><script>self.__next_f.push([1,"7:[\"$\",\"div\",null,{\"className\":\"max-w-4xl mx-auto px-4 sm:px-6 lg:px-8 py-12\",\"children\":[[\"$\",\"$L15\",null,{\"config\":{\"type\":\"publication\",\"title\":\"Publications\",\"description\":\"A collection of my research work.\",\"source\":\"publications.bib\"},\"publications\":[{\"id\":\"VACoT\",\"title\":\"VACoT: Rethinking Visual Data Augmentation with VLMs\",\"authors\":[{\"name\":\"Zhengzhuo Xu\",\"isHighlighted\":true,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Chong Sun\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"SiNan Du\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Chen Li\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Jing LYU\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Chun Yuan\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false}],\"year\":2025,\"month\":\"12\",\"type\":\"journal\",\"status\":\"published\",\"tags\":[\"Visual Augmentation\",\"Chain-of-Thought\",\"VACoT\",\"Visual Language Models (VLMs)\"],\"keywords\":\"$7:props:children:0:props:publications:0:tags\",\"researchArea\":\"machine-learning\",\"journal\":\"Arxiv preprint\",\"conference\":\"\",\"volume\":\"abs/2512.02361\",\"doi\":\"10.48550/ARXIV.2512.02361\",\"url\":\"https://arxiv.org/abs/2512.02361\",\"abstract\":\"\",\"description\":\"A research on Visual Augmentation Chain-of-Thought, a framework that dynamically applies visual augmentations during VLM inference to enhance robustness on challenging/out-of-distribution inputs. It integrates diverse general augmentations (beyond local cropping) with efficient agentic reinforcement learning \u0026 conditional rewards, achieving superior performance on 13 perception benchmarks and introducing AdvOCR to verify post-hoc augmentation benefits.\",\"selected\":false,\"preview\":\"VACoT.png\",\"bibtex\":\"@article{VACoT,\\n  title = {VACoT: Rethinking Visual Data Augmentation with VLMs},\\n  journal = {Arxiv preprint},\\n  author = {Zhengzhuo Xu and Chong Sun and SiNan Du and Chen Li and Jing LYU  and Chun Yuan},\\n  year = {2025},\\n  month = {dec},\\n  volume = {abs/2512.02361},\\n  archivePrefix = {arXiv},\\n  primaryClass = {cs.CV},\\n  url = {https://arxiv.org/abs/2512.02361},\\n  doi = {10.48550/ARXIV.2512.02361},\\n  abstract = {}\\n}\"},{\"id\":\"VQRAE\",\"title\":\"VQRAE: Representation Quantization Autoencoders for Multimodal Understanding, Generation and Reconstruction\",\"authors\":[{\"name\":\"Sinan Du\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Jiahao Guo\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Bo Li\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Shuhao Cui\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Zhengzhuo Xu\",\"isHighlighted\":true,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Yifu Luo\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Yongxian Wei\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Kun Gai\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Xinggang Wang\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Kai Wu\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Chun Yuan\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false}],\"year\":2025,\"month\":\"11\",\"type\":\"journal\",\"status\":\"published\",\"tags\":[\"Multimodal Unified Model\",\"Vector Quantization\",\"Representation AutoEncoder\",\"VQRAE\",\"Visual Understanding\",\"Visual Generation\",\"Image Reconstruction\"],\"keywords\":\"$7:props:children:0:props:publications:1:tags\",\"researchArea\":\"machine-learning\",\"journal\":\"Arxiv preprint\",\"conference\":\"\",\"volume\":\"abs/2511.23386\",\"doi\":\"10.48550/ARXIV.2511.23386\",\"url\":\"https://arxiv.org/abs/2511.23386\",\"abstract\":\"\",\"description\":\"A research on VQRAE, a Vector Quantization-based Representation AutoEncoder that unifies multimodal understanding, generation and reconstruction in a single tokenizer: it produces continuous semantic features for image understanding and discrete tokens for visual generation via a two-stage training strategy (high-dimensional VQ codebook learning + encoder self-distillation), achieving high codebook utilization and competitive performance across visual tasks.\",\"selected\":false,\"preview\":\"VQRAE.png\",\"bibtex\":\"@article{VQRAE,\\n  title = {VQRAE: Representation Quantization Autoencoders for Multimodal Understanding, Generation and Reconstruction},\\n  journal = {Arxiv preprint},\\n  author = {Sinan Du and Jiahao Guo and Bo Li and Shuhao Cui and Zhengzhuo Xu and Yifu Luo and Yongxian Wei and Kun Gai and Xinggang Wang and Kai Wu and Chun Yuan},\\n  year = {2025},\\n  month = {nov},\\n  volume = {abs/2511.23386},\\n  archivePrefix = {arXiv},\\n  primaryClass = {cs.CV},\\n  url = {https://arxiv.org/abs/2511.23386},\\n  doi = {10.48550/ARXIV.2511.23386},\\n  abstract = {}\\n}\"},{\"id\":\"ChartPoint\",\"title\":\"ChartPoint: Guiding MLLMs with Grounding Reflection for Chart Reasoning\",\"authors\":[{\"name\":\"Zhengzhuo Xu\",\"isHighlighted\":true,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Sinan Du\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Yiyan Qi\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Siwen Lu\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Chengjin Xu\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Chun Yuan\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Jian Guo\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false}],\"year\":2025,\"month\":\"9\",\"type\":\"conference\",\"status\":\"published\",\"tags\":[\"Chart Understanding\",\"Mixture of Experts\",\"MoE\",\"Visual Reasoning\",\"Multimodal Learning\"],\"keywords\":\"$7:props:children:0:props:publications:2:tags\",\"researchArea\":\"machine-learning\",\"journal\":\"\",\"conference\":\"IEEE/CVF International Conference on Computer Vision, ICCV\",\"volume\":\"pp 426--436\",\"abstract\":\"\",\"description\":\"A research on PointCoT, a framework integrating reflective interaction into chain-of-thought reasoning, designed to strengthen multimodal large language models' chart comprehension by linking textual reasoning with visual grounding, addressing numerical hallucinations and weak element grounding issues.\",\"selected\":true,\"preview\":\"ChartPoint.png\",\"bibtex\":\"@inproceedings{ChartPoint,\\n  title = {ChartPoint: Guiding MLLMs with Grounding Reflection for Chart Reasoning},\\n  booktitle = {{IEEE/CVF} International Conference on Computer Vision, {ICCV}},\\n  volume = {pp 426--436},\\n  author = {Zhengzhuo Xu and Sinan Du and Yiyan Qi and Siwen Lu and Chengjin Xu and Chun Yuan and Jian Guo},\\n  year = {2025},\\n  month = {sep},\\n  publisher = {{IEEE}},\\n  abstract = {}\\n}\"},{\"id\":\"Seed1.5VL\",\"title\":\"Seed1.5-VL Technical Report\",\"authors\":[{\"name\":\"Dong Guo\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Faming Wu\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Feida Zhu\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Fuxing Leng\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Guang Shi\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Haobin Chen\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Haoqi Fan\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Jian Wang\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Jianyu Jiang\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Jiawei Wang\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Zhengzhuo Xu\",\"isHighlighted\":true,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"others\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false}],\"year\":2025,\"month\":\"5\",\"type\":\"journal\",\"status\":\"published\",\"tags\":[\"Multimodal Model\",\"Visual-Language Learning\",\"Seed1.5-VL\"],\"keywords\":\"$7:props:children:0:props:publications:3:tags\",\"researchArea\":\"machine-learning\",\"journal\":\"Arxiv preprint\",\"conference\":\"\",\"volume\":\"abs/2505.07062\",\"doi\":\"10.48550/ARXIV.2505.07062\",\"abstract\":\"\",\"description\":\"A technical report on Seed1.5-VL, a multimodal model focusing on visual-language tasks, detailing the model architecture, training strategies, and performance evaluations across diverse multimodal benchmarks.\",\"selected\":true,\"preview\":\"Seed1.5VL.png\",\"bibtex\":\"@article{Seed1.5VL,\\n  title = {Seed1.5-VL Technical Report},\\n  journal = {Arxiv preprint},\\n  volume = {abs/2505.07062},\\n  author = {Dong Guo and Faming Wu and Feida Zhu and Fuxing Leng and Guang Shi and Haobin Chen and Haoqi Fan and Jian Wang and Jianyu Jiang and Jiawei Wang and Zhengzhuo Xu and others},\\n  year = {2025},\\n  month = {may},\\n  publisher = {arXiv},\\n  address = {Cornell University Library},\\n  doi = {10.48550/ARXIV.2505.07062},\\n  urldate = {2025-11-14},\\n  issn = {1932-4553},\\n  eprinttype = {arXiv},\\n  eprint = {2505.07062},\\n  abstract = {}\\n}\"},{\"id\":\"ChartMoE\",\"title\":\"ChartMoE: Mixture of Expert Connector for Advanced Chart Understanding\",\"authors\":[{\"name\":\"Zhengzhuo Xu\",\"isHighlighted\":true,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Bowen Qu\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Yiyan Qi\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Sinan Du\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Chengjin Xu\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Chun Yuan\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Jian Guo\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false}],\"year\":2025,\"month\":\"4\",\"type\":\"conference\",\"status\":\"published\",\"tags\":[\"Chart Understanding\",\"Mixture of Experts\",\"MoE\",\"Visual Reasoning\",\"Multimodal Learning\"],\"keywords\":\"$7:props:children:0:props:publications:4:tags\",\"researchArea\":\"machine-learning\",\"journal\":\"\",\"conference\":\"International Conference on Learning Representations, ICLR oral\",\"volume\":\"pp 1--12\",\"abstract\":\"\",\"description\":\"A research on ChartMoE, a Mixture of Expert Connector framework designed to enhance advanced chart understanding capabilities, leveraging expert specialization to address complex visual and semantic reasoning challenges in chart data analysis.\",\"selected\":true,\"preview\":\"ChartMoE.png\",\"bibtex\":\"@inproceedings{ChartMoE,\\n  title = {ChartMoE: Mixture of Expert Connector for Advanced Chart Understanding},\\n  booktitle = {International Conference on Learning Representations, ICLR oral},\\n  volume = {pp 1--12},\\n  author = {Zhengzhuo Xu and Bowen Qu and Yiyan Qi and Sinan Du and Chengjin Xu and Chun Yuan and Jian Guo},\\n  year = {2025},\\n  month = {apr},\\n  publisher = {OpenReview.net},\\n  urldate = {2024-10-09},\\n  issn = {1932-4553},\\n  abstract = {}\\n}\"},{\"id\":\"HLC\",\"title\":\"Boosting Long-Tailed Recognition With Label Descriptor and Beyond\",\"authors\":[{\"name\":\"Zhengzhuo Xu\",\"isHighlighted\":true,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Ruikang Liu\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Zenghao Chai\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Yiyan Qi\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Lei Li\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Haiqin Yang\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Chun Yuan\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false}],\"year\":2025,\"type\":\"journal\",\"status\":\"published\",\"tags\":[\"Long-Tailed Recognition\",\"Label Descriptor\",\"Visual Recognition\",\"Class Imbalance\",\"Model Optimization\"],\"keywords\":\"$7:props:children:0:props:publications:5:tags\",\"researchArea\":\"machine-learning\",\"journal\":\"IEEE Trans. Multim.\",\"conference\":\"\",\"volume\":\"27\",\"pages\":\"8618--8627\",\"doi\":\"10.1109/TMM.2025.3607812\",\"abstract\":\"\",\"description\":\"A study focusing on optimizing long-tailed recognition tasks, introducing label descriptor - based technologies and other auxiliary means to alleviate the impact of data class imbalance on model training, which is of great significance for promoting the practical application of recognition models in real - world scenarios with unbalanced data distribution.\",\"selected\":false,\"preview\":\"HLC.png\",\"bibtex\":\"@article{HLC,\\n  title = {Boosting Long-Tailed Recognition With Label Descriptor and Beyond},\\n  journal = {{IEEE} Trans. Multim.},\\n  author = {Zhengzhuo Xu and Ruikang Liu and Zenghao Chai and Yiyan Qi and Lei Li and Haiqin Yang and Chun Yuan},\\n  year = {2025},\\n  volume = {27},\\n  pages = {8618--8627},\\n  publisher = {{IEEE}},\\n  doi = {10.1109/TMM.2025.3607812},\\n  urldate = {2025-11-25},\\n  issn = {1520-9210},\\n  abstract = {}\\n}\"},{\"id\":\"ALoRE\",\"title\":\"ALoRE: Efficient Visual Adaptation via Aggregating Low Rank Experts\",\"authors\":[{\"name\":\"Sinan Du\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Guosheng Zhang\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Keyao Wang\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Yuanrui Wang\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Haixiao Yue\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Gang Zhang\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Errui Ding\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Jingdong Wang\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Zhengzhuo Xu\",\"isHighlighted\":true,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Chun Yuan\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false}],\"year\":2024,\"month\":\"12\",\"type\":\"journal\",\"status\":\"published\",\"tags\":[\"Visual Adaptation\",\"Low Rank Experts\",\"ALoRE\",\"Efficient Learning\"],\"keywords\":\"$7:props:children:0:props:publications:6:tags\",\"researchArea\":\"machine-learning\",\"journal\":\"Arxiv preprint\",\"conference\":\"\",\"volume\":\"abs/2412.08341\",\"doi\":\"10.48550/ARXIV.2412.08341\",\"abstract\":\"\",\"description\":\"A research on ALoRE framework for efficient visual adaptation, proposing to aggregate low rank experts to reduce the computational overhead of visual model adaptation while maintaining performance in computer vision tasks.\",\"selected\":false,\"preview\":\"ALore.png\",\"bibtex\":\"@article{ALoRE,\\n  title = {ALoRE: Efficient Visual Adaptation via Aggregating Low Rank Experts},\\n  journal = {Arxiv preprint},\\n  volume = {abs/2412.08341},\\n  author = {Sinan Du and Guosheng Zhang and Keyao Wang and Yuanrui Wang and Haixiao Yue and Gang Zhang and Errui Ding and Jingdong Wang and Zhengzhuo Xu and Chun Yuan},\\n  year = {2024},\\n  month = {dec},\\n  publisher = {arXiv},\\n  address = {Cornell University Library},\\n  doi = {10.48550/ARXIV.2412.08341},\\n  urldate = {2025-11-16},\\n  issn = {1932-4553},\\n  eprinttype = {arXiv},\\n  eprint = {2412.08341},\\n  abstract = {}\\n}\"},{\"id\":\"IntactKV\",\"title\":\"IntactKV: Improving Large Language Model Quantization by Keeping Pivot Tokens Intact\",\"authors\":[{\"name\":\"Ruikang Liu\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Haoli Bai\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Haokun Lin\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Yuening Li\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Han Gao\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Zhengzhuo Xu\",\"isHighlighted\":true,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Lu Hou\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Jun Yao\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Chun Yuan\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false}],\"year\":2024,\"month\":\"8\",\"type\":\"conference\",\"status\":\"published\",\"tags\":[\"LLM Quantization\",\"Pivot Tokens\",\"IntactKV\",\"KV Cache\"],\"keywords\":\"$7:props:children:0:props:publications:7:tags\",\"researchArea\":\"machine-learning\",\"journal\":\"\",\"conference\":\"Findings of the Association for Computational Linguistics, ACL\",\"pages\":\"7716--7741\",\"doi\":\"10.18653/V1/2024.FINDINGS-ACL.460\",\"abstract\":\"\",\"description\":\"A research on IntactKV framework for large language model quantization, proposing to keep pivot tokens intact to reduce quantization errors and enhance the precision of quantized LLMs in natural language processing tasks.\",\"selected\":false,\"preview\":\"IntactKV.png\",\"bibtex\":\"@inproceedings{IntactKV,\\n  title = {IntactKV: Improving Large Language Model Quantization by Keeping Pivot Tokens Intact},\\n  booktitle = {{Findings of the Association for Computational Linguistics}, {ACL}},\\n  editor = {Lun{-}Wei Ku and Andre Martins and Vivek Srikumar},\\n  author = {Ruikang Liu and Haoli Bai and Haokun Lin and Yuening Li and Han Gao and Zhengzhuo Xu and Lu Hou and Jun Yao and Chun Yuan},\\n  year = {2024},\\n  month = {aug},\\n  pages = {7716--7741},\\n  publisher = {Association for Computational Linguistics},\\n  address = {Bangkok, Thailand},\\n  doi = {10.18653/V1/2024.FINDINGS-ACL.460},\\n  urldate = {2025-11-26},\\n  isbn = {978-1-959429-17-1},\\n  abstract = {}\\n}\"},{\"id\":\"ECL\",\"title\":\"Towards Effective Collaborative Learning in Long-Tailed Recognition\",\"authors\":[{\"name\":\"Zhengzhuo Xu\",\"isHighlighted\":true,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Zenghao Chai\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Chengyin Xu\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Chun Yuan\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Haiqin Yang\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false}],\"year\":2024,\"type\":\"journal\",\"status\":\"published\",\"tags\":[\"Long-Tailed Recognition\",\"Collaborative Learning\",\"Class Imbalance\",\"Multimedia Processing\",\"Computer Vision\"],\"keywords\":\"$7:props:children:0:props:publications:8:tags\",\"researchArea\":\"signal-processing\",\"journal\":\"IEEE Trans. Multim.\",\"conference\":\"\",\"volume\":\"26\",\"pages\":\"3754--3764\",\"doi\":\"10.1109/TMM.2023.3314980\",\"abstract\":\"\",\"description\":\"A research on effective collaborative learning strategies for long-tailed recognition tasks, exploring collaborative mechanisms to address the class imbalance problem in multimedia and computer vision scenarios.\",\"selected\":false,\"preview\":\"ECL.png\",\"bibtex\":\"@article{ECL,\\n  title = {Towards Effective Collaborative Learning in Long-Tailed Recognition},\\n  journal = {{IEEE} Trans. Multim.},\\n  volume = {26},\\n  author = {Zhengzhuo Xu and Zenghao Chai and Chengyin Xu and Chun Yuan and Haiqin Yang},\\n  year = {2024},\\n  pages = {3754--3764},\\n  publisher = {{IEEE}},\\n  doi = {10.1109/TMM.2023.3314980},\\n  urldate = {2025-11-15},\\n  issn = {1520-9210},\\n  abstract = {}\\n}\"},{\"id\":\"ChartBench\",\"title\":\"ChartBench: A Benchmark for Complex Visual Reasoning in Charts\",\"authors\":[{\"name\":\"Zhengzhuo Xu\",\"isHighlighted\":true,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Sinan Du\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Yiyan Qi\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Chengjin Xu\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Chun Yuan\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Jian Guo\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false}],\"year\":2023,\"month\":\"12\",\"type\":\"journal\",\"status\":\"published\",\"tags\":[\"Visual Reasoning\",\"Chart Analysis\",\"Benchmark\",\"Multimodal Learning\"],\"keywords\":\"$7:props:children:0:props:publications:9:tags\",\"researchArea\":\"machine-learning\",\"journal\":\"Arxiv preprint\",\"conference\":\"\",\"volume\":\"abs/2312.15915\",\"doi\":\"10.48550/ARXIV.2312.15915\",\"abstract\":\"\",\"description\":\"A research on ChartBench, a novel benchmark designed to evaluate the complex visual reasoning capabilities of models on chart data, covering diverse chart types and reasoning tasks in visual analytics.\",\"selected\":true,\"preview\":\"ChartBench.png\",\"bibtex\":\"@article{ChartBench,\\n  title = {ChartBench: {A} Benchmark for Complex Visual Reasoning in Charts},\\n  journal = {Arxiv preprint},\\n  volume = {abs/2312.15915},\\n  author = {Zhengzhuo Xu and Sinan Du and Yiyan Qi and Chengjin Xu and Chun Yuan and Jian Guo},\\n  year = {2023},\\n  month = {dec},\\n  publisher = {arXiv},\\n  address = {Cornell University Library},\\n  doi = {10.48550/ARXIV.2312.15915},\\n  urldate = {2024-01-16},\\n  issn = {1932-4553},\\n  eprinttype = {arXiv},\\n  eprint = {2312.15915},\\n  abstract = {}\\n}\"},{\"id\":\"TokenFace\",\"title\":\"Accurate 3D Face Reconstruction with Facial Component Tokens\",\"authors\":[{\"name\":\"Tianke Zhang\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Xuangeng Chu\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Yunfei Liu\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Lijian Lin\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Zhendong Yang\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Zhengzhuo Xu\",\"isHighlighted\":true,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Chengkun Cao\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Fei Yu\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Changyin Zhou\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Chun Yuan\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Yu Li\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false}],\"year\":2023,\"month\":\"10\",\"type\":\"conference\",\"status\":\"published\",\"tags\":[\"3D Face Reconstruction\",\"Facial Component Tokens\",\"Face Analysis\",\"3D Vision\"],\"keywords\":\"$7:props:children:0:props:publications:10:tags\",\"researchArea\":\"machine-learning\",\"journal\":\"\",\"conference\":\"IEEE/CVF International Conference on Computer Vision, ICCV\",\"pages\":\"8999--9008\",\"doi\":\"10.1109/ICCV51070.2023.00829\",\"abstract\":\"\",\"description\":\"A research on accurate 3D face reconstruction leveraging facial component tokens, proposing a novel token-based approach to enhance the precision and detail of 3D facial structure reconstruction in computer vision tasks.\",\"selected\":false,\"preview\":\"TokenFace.png\",\"bibtex\":\"@inproceedings{TokenFace,\\n  title = {Accurate 3D Face Reconstruction with Facial Component Tokens},\\n  booktitle = {{IEEE/CVF} International Conference on Computer Vision, {ICCV}},\\n  author = {Tianke Zhang and Xuangeng Chu and Yunfei Liu and Lijian Lin and Zhendong Yang and Zhengzhuo Xu and Chengkun Cao and Fei Yu and Changyin Zhou and Chun Yuan and Yu Li},\\n  year = {2023},\\n  month = {oct},\\n  pages = {8999--9008},\\n  publisher = {{IEEE}},\\n  address = {Paris, France},\\n  doi = {10.1109/ICCV51070.2023.00829},\\n  urldate = {2025-11-17},\\n  isbn = {978-1-6654-8921-5},\\n  abstract = {}\\n}\"},{\"id\":\"LiVT\",\"title\":\"Learning Imbalanced Data with Vision Transformers\",\"authors\":[{\"name\":\"Zhengzhuo Xu\",\"isHighlighted\":true,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Ruikang Liu\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Shuo Yang\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Zenghao Chai\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Chun Yuan\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false}],\"year\":2023,\"month\":\"6\",\"type\":\"conference\",\"status\":\"published\",\"tags\":[\"Imbalanced Data Learning\",\"Vision Transformer\",\"ViT\",\"Class Imbalance\"],\"keywords\":\"$7:props:children:0:props:publications:11:tags\",\"researchArea\":\"transformer-architectures\",\"journal\":\"\",\"conference\":\"IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR\",\"pages\":\"15793--15803\",\"doi\":\"10.1109/CVPR52729.2023.01516\",\"abstract\":\"\",\"description\":\"A research on learning imbalanced data with Vision Transformer models, proposing novel strategies to address the class imbalance challenge in computer vision tasks using ViT architectures.\",\"selected\":false,\"preview\":\"LiVT.png\",\"bibtex\":\"@inproceedings{LiVT,\\n  title = {Learning Imbalanced Data with Vision Transformers},\\n  booktitle = {{IEEE/CVF} Conference on Computer Vision and Pattern Recognition, {CVPR}},\\n  author = {Zhengzhuo Xu and Ruikang Liu and Shuo Yang and Zenghao Chai and Chun Yuan},\\n  year = {2023},\\n  month = {jun},\\n  pages = {15793--15803},\\n  publisher = {{IEEE}},\\n  address = {Vancouver, BC, Canada},\\n  doi = {10.1109/CVPR52729.2023.01516},\\n  urldate = {2025-11-26},\\n  isbn = {978-1-6654-8818-8},\\n  abstract = {}\\n}\"},{\"id\":\"RethinkLT\",\"title\":\"Rethink Long-Tailed Recognition with Vision Transforms\",\"authors\":[{\"name\":\"Zhengzhuo Xu\",\"isHighlighted\":true,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Shuo Yang\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Xingjun Wang\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Chun Yuan\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false}],\"year\":2023,\"month\":\"5\",\"type\":\"conference\",\"status\":\"published\",\"tags\":[\"Long-Tailed Recognition\",\"Vision Transformer\",\"ViT\",\"Signal Processing\",\"Visual Recognition\"],\"keywords\":\"$7:props:children:0:props:publications:12:tags\",\"researchArea\":\"signal-processing\",\"journal\":\"\",\"conference\":\"International Conference on Acoustics, Speech and Signal Processing ICASSP\",\"pages\":\"1--5\",\"doi\":\"10.1109/ICASSP49357.2023.10096734\",\"abstract\":\"\",\"description\":\"A research rethinking long-tailed recognition tasks with Vision Transformer models, exploring the advantages of ViT in addressing the class imbalance problem in long-tailed visual recognition scenarios.\",\"selected\":false,\"preview\":\"RethinkLT.png\",\"bibtex\":\"@inproceedings{RethinkLT,\\n  title = {Rethink Long-Tailed Recognition with Vision Transforms},\\n  booktitle = {International Conference on Acoustics, Speech and Signal Processing {ICASSP}},\\n  author = {Zhengzhuo Xu and Shuo Yang and Xingjun Wang and Chun Yuan},\\n  year = {2023},\\n  month = {may},\\n  pages = {1--5},\\n  publisher = {{IEEE}},\\n  address = {Toronto, Canada},\\n  doi = {10.1109/ICASSP49357.2023.10096734},\\n  urldate = {2025-11-30},\\n  isbn = {978-1-6654-7418-6},\\n  abstract = {}\\n}\"},{\"id\":\"LNet\",\"title\":\"A Lightweight Approach for Network Intrusion Detection Based on Self-Knowledge Distillation\",\"authors\":[{\"name\":\"Shuo Yang\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Xinran Zheng\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Zhengzhuo Xu\",\"isHighlighted\":true,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Xingjun Wang\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false}],\"year\":2023,\"month\":\"5\",\"type\":\"conference\",\"status\":\"published\",\"tags\":[\"Network Intrusion Detection\",\"Self-Knowledge Distillation\",\"Lightweight Model\",\"Communication Networks\",\"Cybersecurity\"],\"keywords\":\"$7:props:children:0:props:publications:13:tags\",\"researchArea\":\"machine-learning\",\"journal\":\"\",\"conference\":\"IEEE International Conference on Communications, ICC\",\"pages\":\"3000--3005\",\"doi\":\"10.1109/ICC45041.2023.10279691\",\"abstract\":\"\",\"description\":\"A research on a lightweight network intrusion detection approach based on self-knowledge distillation, designed to enhance the efficiency and accuracy of intrusion detection in communication networks.\",\"selected\":false,\"preview\":\"LNet.png\",\"bibtex\":\"@inproceedings{LNet,\\n  title = {A Lightweight Approach for Network Intrusion Detection Based on Self-Knowledge Distillation},\\n  booktitle = {{IEEE} International Conference on Communications, {ICC}},\\n  author = {Shuo Yang and Xinran Zheng and Zhengzhuo Xu and Xingjun Wang},\\n  year = {2023},\\n  month = {may},\\n  pages = {3000--3005},\\n  publisher = {{IEEE}},\\n  address = {Rome, Italy},\\n  doi = {10.1109/ICC45041.2023.10279691},\\n  urldate = {2024-09-05},\\n  isbn = {978-1-6654-7500-8},\\n  abstract = {}\\n}\"},{\"id\":\"HHF\",\"title\":\"HHF: Hashing-Guided Hinge Function for Deep Hashing Retrieval\",\"authors\":[{\"name\":\"Chengyin Xu\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Zenghao Chai\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Zhengzhuo Xu\",\"isHighlighted\":true,\"isCorresponding\":false,\"isCoAuthor\":true},{\"name\":\"Hongjia Li\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Qiruyi Zuo\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Lingyu Yang\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Chun Yuan\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false}],\"year\":2023,\"type\":\"journal\",\"status\":\"published\",\"tags\":[\"Deep Hashing\",\"Image Retrieval\",\"Hinge Function\",\"Hashing Guidance\",\"Multimedia Retrieval\"],\"keywords\":\"$7:props:children:0:props:publications:14:tags\",\"researchArea\":\"machine-learning\",\"journal\":\"IEEE Trans. Multim.\",\"conference\":\"\",\"volume\":\"25\",\"pages\":\"7428--7440\",\"doi\":\"10.1109/TMM.2022.3222598\",\"abstract\":\"\",\"description\":\"A research on the HHF (Hashing-Guided Hinge Function) framework for deep hashing retrieval, leveraging hashing guidance to optimize hinge functions and enhance the efficiency and accuracy of deep hashing-based retrieval tasks.\",\"selected\":false,\"preview\":\"HHF.png\",\"bibtex\":\"@article{HHF,\\n  title = {{HHF:} Hashing-Guided Hinge Function for Deep Hashing Retrieval},\\n  journal = {{IEEE} Trans. Multim.},\\n  volume = {25},\\n  author = {Chengyin Xu and Zenghao Chai and Zhengzhuo Xu and Hongjia Li and Qiruyi Zuo and Lingyu Yang and Chun Yuan},\\n  year = {2023},\\n  pages = {7428--7440},\\n  publisher = {{IEEE}},\\n  doi = {10.1109/TMM.2022.3222598},\\n  urldate = {2025-11-15},\\n  issn = {1520-9210},\\n  abstract = {}\\n}\"},{\"id\":\"HPYLoss\",\"title\":\"HyP^2 Loss: Beyond Hypersphere Metric Space for Multi-label Image Retrieval\",\"authors\":[{\"name\":\"Chengyin Xu\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Zenghao Chai\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Zhengzhuo Xu\",\"isHighlighted\":true,\"isCorresponding\":false,\"isCoAuthor\":true},{\"name\":\"Chun Yuan\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Yanbo Fan\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Jue Wang\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false}],\"year\":2022,\"month\":\"10\",\"type\":\"conference\",\"status\":\"published\",\"tags\":[\"Multi-label Image Retrieval\",\"Metric Space\",\"Loss Function\",\"Hypersphere\",\"Multimedia Computing\"],\"keywords\":\"$7:props:children:0:props:publications:15:tags\",\"researchArea\":\"machine-learning\",\"journal\":\"\",\"conference\":\"ACM International Conference on Multimedia, ACM MM\",\"pages\":\"3173--3184\",\"doi\":\"10.1145/3503161.3548032\",\"abstract\":\"\",\"description\":\"A research on the HyP(^mbox2) Loss function that breaks through the limitations of the hypersphere metric space, aiming to improve the performance of multi-label image retrieval tasks in multimedia processing.\",\"selected\":false,\"preview\":\"HpyLoss.png\",\"bibtex\":\"@inproceedings{HPYLoss,\\n  title = {HyP^2 Loss: Beyond Hypersphere Metric Space for Multi-label Image Retrieval},\\n  booktitle = {{ACM} International Conference on Multimedia, ACM MM},\\n  editor = {Jo{\\\\~{a}}o Magalh{\\\\~{a}}es and Alberto Del Bimbo and Shin'ichi Satoh and Nicu Sebe and Xavier Alameda{-}Pineda and Qin Jin and Vincent Oria and Laura Toni},\\n  author = {Chengyin Xu and Zenghao Chai and Zhengzhuo Xu and Chun Yuan and Yanbo Fan and Jue Wang},\\n  year = {2022},\\n  month = {oct},\\n  pages = {3173--3184},\\n  publisher = {{ACM}},\\n  address = {Lisboa, Portugal},\\n  doi = {10.1145/3503161.3548032},\\n  urldate = {2025-11-17},\\n  isbn = {978-1-4503-9203-7},\\n  abstract = {}\\n}\"},{\"id\":\"Realy\",\"title\":\"REALY: Rethinking the Evaluation of 3D Face Reconstruction\",\"authors\":[{\"name\":\"Zenghao Chai\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Haoxian Zhang\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Jing Ren\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Di Kang\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Zhengzhuo Xu\",\"isHighlighted\":true,\"isCorresponding\":false,\"isCoAuthor\":true},{\"name\":\"Xuefei Zhe\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Chun Yuan\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Linchao Bao\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false}],\"year\":2022,\"month\":\"10\",\"type\":\"conference\",\"status\":\"published\",\"tags\":[\"3D Face Reconstruction\",\"Evaluation Metrics\",\"Computer Vision\",\"Face Analysis\",\"REALY Framework\"],\"keywords\":\"$7:props:children:0:props:publications:16:tags\",\"researchArea\":\"machine-learning\",\"journal\":\"\",\"conference\":\"European Conference Computer Vision, ECCV\",\"volume\":\"13668\",\"pages\":\"74--92\",\"doi\":\"10.1007/978-3-031-20074-8_5\",\"abstract\":\"\",\"description\":\"A research on REALY framework that rethinks the evaluation metrics and methodologies for 3D face reconstruction, aiming to provide more accurate and comprehensive evaluation criteria for the field.\",\"selected\":false,\"preview\":\"REALY.png\",\"bibtex\":\"@inproceedings{Realy,\\n  title = {{REALY}: Rethinking the Evaluation of 3D Face Reconstruction},\\n  booktitle = {European Conference Computer Vision, {ECCV}},\\n  series = {Lecture Notes in Computer Science},\\n  volume = {13668},\\n  editor = {Shai Avidan and Gabriel J. Brostow and Moustapha Ciss{\\\\'{e}} and Giovanni Maria Farinella and Tal Hassner},\\n  author = {Zenghao Chai and Haoxian Zhang and Jing Ren and Di Kang and Zhengzhuo Xu and Xuefei Zhe and Chun Yuan and Linchao Bao},\\n  year = {2022},\\n  month = {oct},\\n  pages = {74--92},\\n  publisher = {Springer},\\n  address = {Tel Aviv, Israel},\\n  doi = {10.1007/978-3-031-20074-8_5},\\n  urldate = {2025-11-15},\\n  isbn = {978-3-031-20074-8},\\n  abstract = {}\\n}\"},{\"id\":\"ColorNet\",\"title\":\"Semantic-Sparse Colorization Network for Deep Exemplar-Based Colorization\",\"authors\":[{\"name\":\"Yunpeng Bai\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Chao Dong\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Zenghao Chai\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Andong Wang\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Zhengzhuo Xu\",\"isHighlighted\":true,\"isCorresponding\":false,\"isCoAuthor\":true},{\"name\":\"Chun Yuan\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false}],\"year\":2022,\"month\":\"10\",\"type\":\"conference\",\"status\":\"published\",\"tags\":[\"Image Colorization\",\"Exemplar-Based Colorization\",\"Semantic-Sparse Network\",\"Computer Vision\",\"Deep Learning\"],\"keywords\":\"$7:props:children:0:props:publications:17:tags\",\"researchArea\":\"machine-learning\",\"journal\":\"\",\"conference\":\"European Conference Computer Vision, ECCV\",\"volume\":\"13666\",\"pages\":\"505--521\",\"doi\":\"10.1007/978-3-031-20068-7_29\",\"abstract\":\"\",\"description\":\"A research on the Semantic-Sparse Colorization Network, proposing a novel deep exemplar-based colorization approach that leverages semantic sparsity to enhance colorization accuracy and realism.\",\"selected\":false,\"preview\":\"ColorNet.png\",\"bibtex\":\"@inproceedings{ColorNet,\\n  title = {Semantic-Sparse Colorization Network for Deep Exemplar-Based Colorization},\\n  booktitle = {European Conference Computer Vision, {ECCV}},\\n  series = {Lecture Notes in Computer Science},\\n  volume = {13666},\\n  editor = {Shai Avidan and Gabriel J. Brostow and Moustapha Ciss{\\\\'{e}} and Giovanni Maria Farinella and Tal Hassner},\\n  author = {Yunpeng Bai and Chao Dong and Zenghao Chai and Andong Wang and Zhengzhuo Xu and Chun Yuan},\\n  year = {2022},\\n  month = {oct},\\n  pages = {505--521},\\n  publisher = {Springer},\\n  address = {Tel Aviv, Israel},\\n  doi = {10.1007/978-3-031-20068-7_29},\\n  urldate = {2025-11-16},\\n  isbn = {978-3-031-20068-7},\\n  abstract = {}\\n}\"},{\"id\":\"CMSLSTM\",\"title\":\"CMS-LSTM: Context Embedding and Multi-Scale Spatiotemporal Expression LSTM for Predictive Learning\",\"authors\":[{\"name\":\"Zenghao Chai\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Zhengzhuo Xu\",\"isHighlighted\":true,\"isCorresponding\":false,\"isCoAuthor\":true},{\"name\":\"Yunpeng Bai\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Zhihui Lin\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Chun Yuan\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false}],\"year\":2022,\"month\":\"7\",\"type\":\"conference\",\"status\":\"published\",\"tags\":[\"Predictive Learning\",\"LSTM\",\"Context Embedding\",\"Multi-Scale Spatiotemporal Analysis\",\"Multimedia Processing\"],\"keywords\":\"$7:props:children:0:props:publications:18:tags\",\"researchArea\":\"signal-processing\",\"journal\":\"\",\"conference\":\"IEEE International Conference on Multimedia and Expo, ICME\",\"pages\":\"1--6\",\"doi\":\"10.1109/ICME52920.2022.9859659\",\"abstract\":\"\",\"description\":\"A study on the CMS-LSTM framework integrating context embedding and multi-scale spatiotemporal expression, designed to boost predictive learning performance in multimedia spatiotemporal tasks.\",\"selected\":false,\"preview\":\"ICME2022.png\",\"bibtex\":\"@inproceedings{CMSLSTM,\\n  title = {{CMS-LSTM}: Context Embedding and Multi-Scale Spatiotemporal Expression {LSTM} for Predictive Learning},\\n  booktitle = {{IEEE} International Conference on Multimedia and Expo, {ICME}},\\n  author = {Zenghao Chai and Zhengzhuo Xu and Yunpeng Bai and Zhihui Lin and Chun Yuan},\\n  year = {2022},\\n  month = {jul},\\n  pages = {1--6},\\n  publisher = {{IEEE}},\\n  address = {Taipei, Taiwan},\\n  doi = {10.1109/ICME52920.2022.9859659},\\n  urldate = {2025-11-16},\\n  isbn = {978-1-6654-8681-7},\\n  abstract = {}\\n}\"},{\"id\":\"MODERNN\",\"title\":\"Modernn: Towards Fine-Grained Motion Details for Spatiotemporal Predictive Learning\",\"authors\":[{\"name\":\"Zenghao Chai\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Zhengzhuo Xu\",\"isHighlighted\":true,\"isCorresponding\":false,\"isCoAuthor\":true},{\"name\":\"Chun Yuan\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false}],\"year\":2022,\"month\":\"5\",\"type\":\"conference\",\"status\":\"published\",\"tags\":[\"Spatiotemporal Learning\",\"Motion Details\",\"Predictive Learning\",\"Signal Processing\",\"Computer Vision\"],\"keywords\":\"$7:props:children:0:props:publications:19:tags\",\"researchArea\":\"signal-processing\",\"journal\":\"\",\"conference\":\"IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP\",\"pages\":\"4658--4662\",\"doi\":\"10.1109/ICASSP43922.2022.9747035\",\"abstract\":\"\",\"description\":\"A research on Modernn framework for capturing fine-grained motion details, aiming to enhance spatiotemporal predictive learning performance in visual signal processing tasks.\",\"selected\":false,\"preview\":\"ICASSP2022.png\",\"bibtex\":\"@inproceedings{MODERNN,\\n  title = {Modernn: Towards Fine-Grained Motion Details for Spatiotemporal Predictive Learning},\\n  booktitle = {{IEEE} International Conference on Acoustics, Speech and Signal Processing, {ICASSP}},\\n  author = {Zenghao Chai and Zhengzhuo Xu and Chun Yuan},\\n  year = {2022},\\n  month = {may},\\n  pages = {4658--4662},\\n  publisher = {{IEEE}},\\n  address = {Singapore (Virtual Event)},\\n  doi = {10.1109/ICASSP43922.2022.9747035},\\n  urldate = {2022-06-07},\\n  isbn = {978-1-6654-2155-6},\\n  abstract = {}\\n}\"},{\"id\":\"PriorLT\",\"title\":\"Towards Calibrated Model for Long-Tailed Visual Recognition from Prior Perspective\",\"authors\":[{\"name\":\"Zhengzhuo Xu\",\"isHighlighted\":true,\"isCorresponding\":false,\"isCoAuthor\":true},{\"name\":\"Zenghao Chai\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Chun Yuan\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false}],\"year\":2021,\"month\":\"12\",\"type\":\"conference\",\"status\":\"published\",\"tags\":[\"Long-Tailed Recognition\",\"Visual Recognition\",\"Model Calibration\",\"Prior Knowledge\",\"Deep Learning\"],\"keywords\":\"$7:props:children:0:props:publications:20:tags\",\"researchArea\":\"machine-learning\",\"journal\":\"\",\"conference\":\"Advances in Neural Information Processing Systems, NeurIPS\",\"pages\":\"7139--7152\",\"doi\":\"10.48550/arXiv.2110.04456\",\"abstract\":\"\",\"description\":\"A research on calibrated models for long-tailed visual recognition from a prior perspective, addressing the calibration and classification challenges in long-tailed visual data scenarios.\",\"selected\":false,\"preview\":\"NIPS2021.png\",\"bibtex\":\"@inproceedings{PriorLT,\\n  title = {Towards Calibrated Model for Long-Tailed Visual Recognition from Prior Perspective},\\n  booktitle = {Advances in Neural Information Processing Systems, NeurIPS},\\n  editor = {Marc'Aurelio Ranzato and Alina Beygelzimer and Yann N. Dauphin and Percy Liang and Jennifer Wortman Vaughan},\\n  author = {Zhengzhuo Xu and Zenghao Chai and Chun Yuan},\\n  year = {2021},\\n  month = {dec},\\n  pages = {7139--7152},\\n  publisher = {Curran Associates, Inc.},\\n  address = {Virtual Event},\\n  doi = {10.48550/arXiv.2110.04456},\\n  urldate = {2022-05-03},\\n  isbn = {978-1-7138-4539-3},\\n  abstract = {}\\n}\"},{\"id\":\"GLOBECOM\",\"title\":\"Backscatter-Assisted Hybrid Relaying Strategy for Wireless Powered IoT Communications\",\"authors\":[{\"name\":\"Yutong Xie\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Zhengzhuo Xu\",\"isHighlighted\":true,\"isCorresponding\":false,\"isCoAuthor\":true},{\"name\":\"Shimin Gong\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Jing Xu\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Dinh Thai Hoang\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Dusit Niyato\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false}],\"year\":2019,\"month\":\"12\",\"type\":\"conference\",\"status\":\"published\",\"tags\":[\"IoT Communications\",\"Backscatter Communication\",\"Wireless Power Transfer\",\"Hybrid Relaying\",\"Wireless Networks\"],\"keywords\":\"$7:props:children:0:props:publications:21:tags\",\"researchArea\":\"machine-learning\",\"journal\":\"\",\"conference\":\"IEEE Global Communications Conference, GLOBECOM\",\"pages\":\"1--6\",\"doi\":\"10.1109/GLOBECOM38437.2019.9013386\",\"abstract\":\"\",\"description\":\"A study on backscatter-assisted hybrid relaying strategies for wireless powered IoT communications, exploring optimized relaying schemes to enhance the performance of IoT communication systems.\",\"selected\":false,\"preview\":\"GLOBECOM.png\",\"bibtex\":\"@inproceedings{GLOBECOM,\\n  title = {Backscatter-Assisted Hybrid Relaying Strategy for Wireless Powered IoT Communications},\\n  booktitle = {{IEEE} Global Communications Conference, {GLOBECOM}},\\n  author = {Yutong Xie and Zhengzhuo Xu and Shimin Gong and Jing Xu and Dinh Thai Hoang and Dusit Niyato},\\n  year = {2019},\\n  month = {dec},\\n  pages = {1--6},\\n  publisher = {{IEEE}},\\n  address = {Waikoloa, HI, USA},\\n  doi = {10.1109/GLOBECOM38437.2019.9013386},\\n  urldate = {2025-02-19},\\n  isbn = {978-1-7281-0962-6},\\n  abstract = {}\\n}\"},{\"id\":\"ICCC\",\"title\":\"Backscatter-Assisted Computation Offloading for Energy Harvesting IoT Devices via Policy-based Deep Reinforcement Learning\",\"authors\":[{\"name\":\"Yutong Xie\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Zhengzhuo Xu\",\"isHighlighted\":true,\"isCorresponding\":false,\"isCoAuthor\":true},{\"name\":\"Yuxing Zhong\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Jing Xu\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Shimin Gong\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Yi Wang\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false}],\"year\":2019,\"month\":\"8\",\"type\":\"conference\",\"status\":\"published\",\"tags\":[\"IoT\",\"Computation Offloading\",\"Backscatter Communication\",\"Energy Harvesting\",\"Deep Reinforcement Learning\"],\"keywords\":\"$7:props:children:0:props:publications:22:tags\",\"researchArea\":\"machine-learning\",\"journal\":\"\",\"conference\":\"IEEE/CIC International Conference on Communications Workshops\",\"pages\":\"65--70\",\"doi\":\"10.1109/ICCChinaW.2019.8849964\",\"abstract\":\"\",\"description\":\"A research on backscatter-assisted computation offloading for energy harvesting IoT devices, adopting policy-based deep reinforcement learning to optimize offloading decisions for energy-constrained IoT scenarios.\",\"selected\":false,\"preview\":\"ICCC.png\",\"bibtex\":\"@inproceedings{ICCC,\\n  title = {Backscatter-Assisted Computation Offloading for Energy Harvesting IoT Devices via Policy-based Deep Reinforcement Learning},\\n  booktitle = {{IEEE/CIC} International Conference on Communications Workshops},\\n  author = {Yutong Xie and Zhengzhuo Xu and Yuxing Zhong and Jing Xu and Shimin Gong and Yi Wang},\\n  year = {2019},\\n  month = {aug},\\n  pages = {65--70},\\n  publisher = {{IEEE}},\\n  address = {Changchun, China},\\n  doi = {10.1109/ICCChinaW.2019.8849964},\\n  urldate = {2025-02-19},\\n  isbn = {978-1-7281-2524-3},\\n  abstract = {}\\n}\"},{\"id\":\"MLICOM\",\"title\":\"Backscatter-Aided Hybrid Data Offloading for Mobile Edge Computing via Deep Reinforcement Learning\",\"authors\":[{\"name\":\"Yutong Xie\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Zhengzhuo Xu\",\"isHighlighted\":true,\"isCorresponding\":false,\"isCoAuthor\":true},{\"name\":\"Jing Xu\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Shimin Gong\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Yi Wang\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false}],\"year\":2019,\"month\":\"8\",\"type\":\"conference\",\"status\":\"published\",\"tags\":[\"Mobile Edge Computing\",\"Data Offloading\",\"Backscatter Communication\",\"Deep Reinforcement Learning\"],\"keywords\":\"$7:props:children:0:props:publications:23:tags\",\"researchArea\":\"machine-learning\",\"journal\":\"\",\"conference\":\"Machine Learning and Intelligent Communications, MLICOM\",\"volume\":\"294\",\"pages\":\"525--537\",\"doi\":\"10.1007/978-3-030-32388-2_45\",\"abstract\":\"\",\"description\":\"A study on backscatter-aided hybrid data offloading for mobile edge computing, leveraging deep reinforcement learning to optimize the offloading strategy.\",\"selected\":false,\"preview\":\"MLICOM.png\",\"bibtex\":\"@inproceedings{MLICOM,\\n  title = {Backscatter-Aided Hybrid Data Offloading for Mobile Edge Computing via Deep Reinforcement Learning},\\n  booktitle = {Machine Learning and Intelligent Communications, {MLICOM}},\\n  series = {Lecture Notes of the Institute for Computer Sciences, Social Informatics and Telecommunications Engineering},\\n  volume = {294},\\n  author = {Yutong Xie and Zhengzhuo Xu and Jing Xu and Shimin Gong and Yi Wang},\\n  editor = {Xiangping Bryce Zhai and Bing Chen and Kun Zhu},\\n  year = {2019},\\n  month = {aug},\\n  pages = {525--537},\\n  publisher = {Springer},\\n  address = {Nanjing, China},\\n  doi = {10.1007/978-3-030-32388-2_45},\\n  urldate = {2025-02-19},\\n  isbn = {978-3-030-32388-2},\\n  abstract = {}\\n}\"}]}],false,false]}]\n"])</script><script>self.__next_f.push([1,"c:null\n"])</script><script>self.__next_f.push([1,"10:[[\"$\",\"meta\",\"0\",{\"charSet\":\"utf-8\"}],[\"$\",\"meta\",\"1\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}]]\nb:null\n"])</script><script>self.__next_f.push([1,"14:{\"metadata\":[[\"$\",\"title\",\"0\",{\"children\":\"Publications | Zhengzhuo Xu (许正卓)\"}],[\"$\",\"meta\",\"1\",{\"name\":\"description\",\"content\":\"A collection of my research work.\"}],[\"$\",\"meta\",\"2\",{\"name\":\"author\",\"content\":\"Zhengzhuo Xu\"}],[\"$\",\"meta\",\"3\",{\"name\":\"keywords\",\"content\":\"Zhengzhuo Xu,PhD,Research,CS Ph.D. Candidate\"}],[\"$\",\"meta\",\"4\",{\"name\":\"creator\",\"content\":\"Zhengzhuo Xu\"}],[\"$\",\"meta\",\"5\",{\"name\":\"publisher\",\"content\":\"Zhengzhuo Xu\"}],[\"$\",\"meta\",\"6\",{\"property\":\"og:title\",\"content\":\"Zhengzhuo Xu (许正卓)\"}],[\"$\",\"meta\",\"7\",{\"property\":\"og:description\",\"content\":\"PhD student at the University of Example.\"}],[\"$\",\"meta\",\"8\",{\"property\":\"og:site_name\",\"content\":\"Zhengzhuo Xu's Academic Website\"}],[\"$\",\"meta\",\"9\",{\"property\":\"og:locale\",\"content\":\"en_US\"}],[\"$\",\"meta\",\"10\",{\"property\":\"og:type\",\"content\":\"website\"}],[\"$\",\"meta\",\"11\",{\"name\":\"twitter:card\",\"content\":\"summary\"}],[\"$\",\"meta\",\"12\",{\"name\":\"twitter:title\",\"content\":\"Zhengzhuo Xu (许正卓)\"}],[\"$\",\"meta\",\"13\",{\"name\":\"twitter:description\",\"content\":\"PhD student at the University of Example.\"}],[\"$\",\"link\",\"14\",{\"rel\":\"icon\",\"href\":\"/favicon.svg\"}]],\"error\":null,\"digest\":\"$undefined\"}\ne:{\"metadata\":\"$14:metadata\",\"error\":null,\"digest\":\"$undefined\"}\n"])</script></body></html>