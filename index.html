<!DOCTYPE html><html lang="en" class="scroll-smooth"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="preload" as="image" href="/bio.jpg"/><link rel="stylesheet" href="/_next/static/css/19bf7477d82f620e.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/_next/static/chunks/webpack-679b75d1c4c2027c.js"/><script src="/_next/static/chunks/4bd1b696-70b6d399998de86a.js" async=""></script><script src="/_next/static/chunks/684-5aaa8290a129f299.js" async=""></script><script src="/_next/static/chunks/main-app-0f9a19a0d5325b39.js" async=""></script><script src="/_next/static/chunks/17-9c04c9413a9f9f1f.js" async=""></script><script src="/_next/static/chunks/874-6cc630662f3664af.js" async=""></script><script src="/_next/static/chunks/862-15038af301665bb3.js" async=""></script><script src="/_next/static/chunks/app/layout-347869ba10f90412.js" async=""></script><script src="/_next/static/chunks/178-595a94b9af1e67b5.js" async=""></script><script src="/_next/static/chunks/748-1f3129a1e6365cf9.js" async=""></script><script src="/_next/static/chunks/app/page-054ab306d65a82c3.js" async=""></script><link rel="icon" href="/favicon.svg" type="image/svg+xml"/><link rel="dns-prefetch" href="https://google-fonts.jialeliu.com"/><link rel="preconnect" href="https://google-fonts.jialeliu.com" crossorigin=""/><link rel="preload" as="style" href="https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700&amp;family=Crimson+Text:ital,wght@0,400;0,600;1,400&amp;display=swap"/><link rel="stylesheet" id="gfonts-css" href="https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700&amp;family=Crimson+Text:ital,wght@0,400;0,600;1,400&amp;display=swap" media="print"/><script>
              (function(){
                var l = document.getElementById('gfonts-css');
                if (!l) return;
                if (l.media !== 'all') {
                  l.addEventListener('load', function(){ try { l.media = 'all'; } catch(e){} });
                }
              })();
            </script><noscript><link rel="stylesheet" href="https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700&amp;family=Crimson+Text:ital,wght@0,400;0,600;1,400&amp;display=swap"/></noscript><script>
              try {
                const theme = localStorage.getItem('theme-storage');
                const parsed = theme ? JSON.parse(theme) : null;
                const setting = parsed?.state?.theme || 'system';
                const prefersDark = typeof window !== 'undefined' && window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches;
                const effective = setting === 'dark' ? 'dark' : (setting === 'light' ? 'light' : (prefersDark ? 'dark' : 'light'));
                var root = document.documentElement;
                root.classList.add(effective);
                root.setAttribute('data-theme', effective);
              } catch (e) {
                var root = document.documentElement;
                root.classList.add('light');
                root.setAttribute('data-theme', 'light');
              }
            </script><title>Zhengzhuo Xu (ËÆ∏Ê≠£Âçì)</title><meta name="description" content="PhD student at the University of Example."/><meta name="author" content="Zhengzhuo Xu"/><meta name="keywords" content="Zhengzhuo Xu,PhD,Research,CS Ph.D. Candidate"/><meta name="creator" content="Zhengzhuo Xu"/><meta name="publisher" content="Zhengzhuo Xu"/><meta property="og:title" content="Zhengzhuo Xu (ËÆ∏Ê≠£Âçì)"/><meta property="og:description" content="PhD student at the University of Example."/><meta property="og:site_name" content="Zhengzhuo Xu&#x27;s Academic Website"/><meta property="og:locale" content="en_US"/><meta property="og:type" content="website"/><meta name="twitter:card" content="summary"/><meta name="twitter:title" content="Zhengzhuo Xu (ËÆ∏Ê≠£Âçì)"/><meta name="twitter:description" content="PhD student at the University of Example."/><link rel="icon" href="/favicon.svg"/><script>document.querySelectorAll('body link[rel="icon"], body link[rel="apple-touch-icon"]').forEach(el => document.head.appendChild(el))</script><script src="/_next/static/chunks/polyfills-42372ed130431b0a.js" noModule=""></script></head><body class="font-sans antialiased"><div style="visibility:hidden"><nav class="fixed top-0 left-0 right-0 z-50" data-headlessui-state=""><div class="transition-all duration-300 ease-out bg-transparent" style="transform:translateY(-100px)"><div class="max-w-7xl mx-auto px-4 sm:px-6 lg:px-8"><div class="flex justify-between items-center h-16 lg:h-20"><div class="flex-shrink-0" tabindex="0"><a class="text-xl lg:text-2xl font-serif font-semibold text-primary hover:text-accent transition-colors duration-200" href="/">Zhengzhuo Xu (ËÆ∏Ê≠£Âçì)</a></div><div class="hidden lg:block"><div class="ml-10 flex items-center space-x-8"><div class="flex items-baseline space-x-8"><a class="relative px-3 py-2 text-sm font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm text-primary" href="/"><span class="relative z-10">About</span><div class="absolute inset-0 bg-accent/10 rounded-lg"></div></a><a class="relative px-3 py-2 text-sm font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm text-neutral-600 hover:text-primary" href="/publications/"><span class="relative z-10">Publications</span></a><a class="relative px-3 py-2 text-sm font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm text-neutral-600 hover:text-primary" href="/career/"><span class="relative z-10">Career</span></a><a class="relative px-3 py-2 text-sm font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm text-neutral-600 hover:text-primary" href="/awards/"><span class="relative z-10">Awards</span></a><a class="relative px-3 py-2 text-sm font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm text-neutral-600 hover:text-primary" href="/services/"><span class="relative z-10">Services</span></a></div><div class="flex items-center justify-center w-10 h-10 rounded-lg border border-neutral-200 dark:border-[rgba(148,163,184,0.24)] bg-background dark:bg-neutral-800"><div class="w-4 h-4 rounded-full bg-neutral-300 animate-pulse"></div></div></div></div><div class="lg:hidden flex items-center space-x-2"><div class="flex items-center justify-center w-10 h-10 rounded-lg border border-neutral-200 dark:border-[rgba(148,163,184,0.24)] bg-background dark:bg-neutral-800"><div class="w-4 h-4 rounded-full bg-neutral-300 animate-pulse"></div></div><button class="inline-flex items-center justify-center p-2 rounded-md text-neutral-600 hover:text-primary hover:bg-neutral-100 dark:hover:bg-neutral-800 focus:outline-none focus:ring-2 focus:ring-inset focus:ring-accent transition-colors duration-200" id="headlessui-disclosure-button-¬´R5pdb¬ª" type="button" aria-expanded="false" data-headlessui-state=""><span class="sr-only">Open main menu</span><div><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="block h-6 w-6"><path stroke-linecap="round" stroke-linejoin="round" d="M3.75 6.75h16.5M3.75 12h16.5m-16.5 5.25h16.5"></path></svg></div></button></div></div></div></div></nav><main class="min-h-screen pt-16 lg:pt-20"><div class="max-w-6xl mx-auto px-4 sm:px-6 lg:px-8 py-8 bg-background min-h-screen"><div class="grid grid-cols-1 lg:grid-cols-3 gap-12"><div class="lg:col-span-1"><div class="sticky top-8" style="opacity:0;transform:translateY(20px)"><div class="w-64 h-64 mx-auto mb-6 rounded-2xl overflow-hidden shadow-lg hover:shadow-xl transition-all duration-200 hover:scale-105"><img alt="Zhengzhuo Xu" width="256" height="256" decoding="async" data-nimg="1" class="w-full h-full object-cover object-[32%_center]" style="color:transparent" src="/bio.jpg"/></div><div class="text-center mb-6"><h1 class="text-3xl font-serif font-bold text-primary mb-2">Zhengzhuo Xu</h1><p class="text-lg text-accent font-medium mb-1">Tsinghua University</p><p class="text-neutral-600 mb-2">CS Ph.D. Candidate</p></div><div class="flex flex-wrap justify-center gap-3 sm:gap-4 mb-6 relative px-2"><div class="relative"><button class="p-2 sm:p-2 transition-colors duration-200 text-neutral-600 dark:text-neutral-400 hover:text-accent" aria-label="Email"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-5 w-5"><path stroke-linecap="round" stroke-linejoin="round" d="M21.75 6.75v10.5a2.25 2.25 0 0 1-2.25 2.25h-15a2.25 2.25 0 0 1-2.25-2.25V6.75m19.5 0A2.25 2.25 0 0 0 19.5 4.5h-15a2.25 2.25 0 0 0-2.25 2.25m19.5 0v.243a2.25 2.25 0 0 1-1.07 1.916l-7.5 4.615a2.25 2.25 0 0 1-2.36 0L3.32 8.91a2.25 2.25 0 0 1-1.07-1.916V6.75"></path></svg></button></div><div class="relative"><button class="p-2 sm:p-2 transition-colors duration-200 text-neutral-600 dark:text-neutral-400 hover:text-accent" aria-label="Location"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-5 w-5"><path stroke-linecap="round" stroke-linejoin="round" d="M15 10.5a3 3 0 1 1-6 0 3 3 0 0 1 6 0Z"></path><path stroke-linecap="round" stroke-linejoin="round" d="M19.5 10.5c0 7.142-7.5 11.25-7.5 11.25S4.5 17.642 4.5 10.5a7.5 7.5 0 1 1 15 0Z"></path></svg></button></div><a href="https://scholar.google.com/citations?hl=zh-CN&amp;user=08hV-X8AAAAJ" target="_blank" rel="noopener noreferrer" class="p-2 sm:p-2 text-neutral-600 dark:text-neutral-400 hover:text-accent transition-colors duration-200" aria-label="Google Scholar"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-5 w-5"><path stroke-linecap="round" stroke-linejoin="round" d="M4.26 10.147a60.438 60.438 0 0 0-.491 6.347A48.62 48.62 0 0 1 12 20.904a48.62 48.62 0 0 1 8.232-4.41 60.46 60.46 0 0 0-.491-6.347m-15.482 0a50.636 50.636 0 0 0-2.658-.813A59.906 59.906 0 0 1 12 3.493a59.903 59.903 0 0 1 10.399 5.84c-.896.248-1.783.52-2.658.814m-15.482 0A50.717 50.717 0 0 1 12 13.489a50.702 50.702 0 0 1 7.74-3.342M6.75 15a.75.75 0 1 0 0-1.5.75.75 0 0 0 0 1.5Zm0 0v-3.675A55.378 55.378 0 0 1 12 8.443m-7.007 11.55A5.981 5.981 0 0 0 6.75 15.75v-1.5"></path></svg></a><a href="https://github.com/XuZhengzhuo/" target="_blank" rel="noopener noreferrer" class="p-2 sm:p-2 text-neutral-600 dark:text-neutral-400 hover:text-accent transition-colors duration-200" aria-label="GitHub"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-github h-5 w-5" aria-hidden="true"><path d="M15 22v-4a4.8 4.8 0 0 0-1-3.5c3 0 6-2 6-5.5.08-1.25-.27-2.48-1-3.5.28-1.15.28-2.35 0-3.5 0 0-1 0-3 1.5-2.64-.5-5.36-.5-8 0C6 2 5 2 5 2c-.3 1.15-.3 2.35 0 3.5A5.403 5.403 0 0 0 4 9c0 3.5 3 5.5 6 5.5-.39.49-.68 1.05-.85 1.65-.17.6-.22 1.23-.15 1.85v4"></path><path d="M9 18c-4.51 2-5-2-7-2"></path></svg></a></div><div class="bg-neutral-100 dark:bg-neutral-800 rounded-lg p-4 mb-6 hover:shadow-lg transition-all duration-200 hover:scale-[1.02]"><h3 class="font-semibold text-primary mb-3">Research Interests</h3><div class="space-y-2 text-sm text-neutral-700 dark:text-neutral-500"><div>Perception and Reasoning</div><div>Vision Language Models</div><div>Long-tailed Visual Recognition</div></div></div><div class="flex justify-center"><div class="relative"><button class="flex items-center space-x-2 px-4 py-2 rounded-lg font-medium text-sm transition-all duration-200 bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-500 hover:bg-red-50 dark:hover:bg-red-900/20 hover:text-red-600 dark:hover:text-red-400 cursor-pointer" tabindex="0"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-4 w-4"><path stroke-linecap="round" stroke-linejoin="round" d="M21 8.25c0-2.485-2.099-4.5-4.688-4.5-1.935 0-3.597 1.126-4.312 2.733-.715-1.607-2.377-2.733-4.313-2.733C5.1 3.75 3 5.765 3 8.25c0 7.22 9 12 9 12s9-4.78 9-12Z"></path></svg><span>Like</span></button></div></div></div></div><div class="lg:col-span-2 space-y-8"><section id="about" class="scroll-mt-24 space-y-8"><section style="opacity:0;transform:translateY(20px)"><h2 class="text-2xl font-serif font-bold text-primary mb-4">About</h2><div class="text-neutral-700 dark:text-neutral-600 leading-relaxed"><p class="mb-4 last:mb-0">I am a CS Ph.D candidate at <a href="https://www.sigs.tsinghua.edu.cn/" node="[object Object]" target="_blank" rel="noopener noreferrer" class="text-accent font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm">SIGS</a>, <a href="https://www.tsinghua.edu.cn/" node="[object Object]" target="_blank" rel="noopener noreferrer" class="text-accent font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm">Tsinghua University</a>, under the supervision of <em class="italic text-neutral-600 dark:text-neutral-500">Prof</em>. <a href="https://www.sigs.tsinghua.edu.cn/yc2/main.htm" node="[object Object]" target="_blank" rel="noopener noreferrer" class="text-accent font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm">Yuan Chun</a>. I graduated with a Bachelor degree of Electrical Engineering from <a href="http://qiming.hust.edu.cn/" node="[object Object]" target="_blank" rel="noopener noreferrer" class="text-accent font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm">Qiming College</a>, <a href="http://ei.hust.edu.cn/" node="[object Object]" target="_blank" rel="noopener noreferrer" class="text-accent font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm">EIC</a>, <a href="https://www.hust.edu.cn/" node="[object Object]" target="_blank" rel="noopener noreferrer" class="text-accent font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm">Huazhong University of Science and Technology</a> in <strong class="font-semibold text-primary">June 2020</strong>, and I commenced my Ph.D studies at Tsinghua University in <strong class="font-semibold text-primary">September 2020</strong>.</p></div></section><section style="opacity:0;transform:translateY(20px)"><h2 class="text-2xl font-serif font-bold text-primary mb-4">News</h2><div class="space-y-3"><div class="flex items-start space-x-3"><span class="text-xs text-neutral-500 mt-1 w-16 flex-shrink-0">2025-07</span><p class="text-sm text-neutral-700">Our work has been accepted by ICCV 2025 üéâ</p></div></div></section><section style="opacity:0;transform:translateY(20px)"><div class="flex items-center justify-between mb-4"><h2 class="text-2xl font-serif font-bold text-primary">Selected Publications</h2><a class="text-accent hover:text-accent-dark text-sm font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm" href="/#publications">View All ‚Üí</a></div><div class="space-y-4"><div class="bg-neutral-50 dark:bg-neutral-800 p-4 rounded-lg shadow-sm border border-neutral-200 dark:border-[rgba(148,163,184,0.24)] hover:shadow-lg transition-all duration-200 hover:scale-[1.02]" style="opacity:0;transform:translateY(20px)"><h3 class="font-semibold text-primary mb-2 leading-tight">ChartPoint: Guiding MLLMs with Grounding Reflection for Chart Reasoning</h3><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-1"><span><span class="font-semibold text-accent">Zhengzhuo Xu</span>, </span><span><span class="">Sinan Du</span>, </span><span><span class="">Yiyan Qi</span>, </span><span><span class="">Siwen Lu</span>, </span><span><span class="">Chengjin Xu</span>, </span><span><span class="">Chun Yuan</span>, </span><span><span class="">Jian Guo</span></span></p><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-2">IEEE/CVF International Conference on Computer Vision, ICCV</p><p class="text-sm text-neutral-500 dark:text-neutral-500 line-clamp-2">A research on ChartMoE, a Mixture of Expert Connector framework designed to enhance advanced chart understanding capabilities, leveraging expert specialization to address complex visual and semantic reasoning challenges in chart data analysis.</p></div><div class="bg-neutral-50 dark:bg-neutral-800 p-4 rounded-lg shadow-sm border border-neutral-200 dark:border-[rgba(148,163,184,0.24)] hover:shadow-lg transition-all duration-200 hover:scale-[1.02]" style="opacity:0;transform:translateY(20px)"><h3 class="font-semibold text-primary mb-2 leading-tight">Seed1.5-VL Technical Report</h3><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-1"><span><span class="">Dong Guo</span>, </span><span><span class="">Faming Wu</span>, </span><span><span class="">Feida Zhu</span>, </span><span><span class="">Fuxing Leng</span>, </span><span><span class="">Guang Shi</span>, </span><span><span class="">Haobin Chen</span>, </span><span><span class="">Haoqi Fan</span>, </span><span><span class="">Jian Wang</span>, </span><span><span class="">Jianyu Jiang</span>, </span><span><span class="">Jiawei Wang</span>, </span><span><span class="font-semibold text-accent">Zhengzhuo Xu</span>, </span><span><span class="">others</span></span></p><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-2">CoRR</p><p class="text-sm text-neutral-500 dark:text-neutral-500 line-clamp-2">A technical report on Seed1.5-VL, a multimodal model focusing on visual-language tasks, detailing the model architecture, training strategies, and performance evaluations across diverse multimodal benchmarks.</p></div><div class="bg-neutral-50 dark:bg-neutral-800 p-4 rounded-lg shadow-sm border border-neutral-200 dark:border-[rgba(148,163,184,0.24)] hover:shadow-lg transition-all duration-200 hover:scale-[1.02]" style="opacity:0;transform:translateY(20px)"><h3 class="font-semibold text-primary mb-2 leading-tight">ChartMoE: Mixture of Expert Connector for Advanced Chart Understanding</h3><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-1"><span><span class="font-semibold text-accent">Zhengzhuo Xu</span>, </span><span><span class="">Bowen Qu</span>, </span><span><span class="">Yiyan Qi</span>, </span><span><span class="">Sinan Du</span>, </span><span><span class="">Chengjin Xu</span>, </span><span><span class="">Chun Yuan</span>, </span><span><span class="">Jian Guo</span></span></p><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-2">International Conference on Learning Representations, ICLR oral</p><p class="text-sm text-neutral-500 dark:text-neutral-500 line-clamp-2">A research on ChartMoE, a Mixture of Expert Connector framework designed to enhance advanced chart understanding capabilities, leveraging expert specialization to address complex visual and semantic reasoning challenges in chart data analysis.</p></div><div class="bg-neutral-50 dark:bg-neutral-800 p-4 rounded-lg shadow-sm border border-neutral-200 dark:border-[rgba(148,163,184,0.24)] hover:shadow-lg transition-all duration-200 hover:scale-[1.02]" style="opacity:0;transform:translateY(20px)"><h3 class="font-semibold text-primary mb-2 leading-tight">Learning Imbalanced Data with Vision Transformers</h3><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-1"><span><span class="font-semibold text-accent">Zhengzhuo Xu</span>, </span><span><span class="">Ruikang Liu</span>, </span><span><span class="">Shuo Yang</span>, </span><span><span class="">Zenghao Chai</span>, </span><span><span class="">Chun Yuan</span></span></p><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-2">IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR</p><p class="text-sm text-neutral-500 dark:text-neutral-500 line-clamp-2">A research on learning imbalanced data with Vision Transformer models, proposing novel strategies to address the class imbalance challenge in computer vision tasks using ViT architectures.</p></div><div class="bg-neutral-50 dark:bg-neutral-800 p-4 rounded-lg shadow-sm border border-neutral-200 dark:border-[rgba(148,163,184,0.24)] hover:shadow-lg transition-all duration-200 hover:scale-[1.02]" style="opacity:0;transform:translateY(20px)"><h3 class="font-semibold text-primary mb-2 leading-tight">Towards Calibrated Model for Long-Tailed Visual Recognition from Prior Perspective</h3><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-1"><span><span class="font-semibold text-accent">Zhengzhuo Xu</span>, </span><span><span class="">Zenghao Chai</span>, </span><span><span class="">Chun Yuan</span></span></p><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-2">Advances in Neural Information Processing Systems, NeurIPS</p><p class="text-sm text-neutral-500 dark:text-neutral-500 line-clamp-2">A research on calibrated models for long-tailed visual recognition from a prior perspective, addressing the calibration and classification challenges in long-tailed visual data scenarios.</p></div></div></section></section></div></div></div><!--$--><!--/$--><!--$--><!--/$--></main><footer class="border-t border-neutral-200/50 bg-neutral-50/50 dark:bg-neutral-900/50 dark:border-neutral-700/50"><div class="max-w-6xl mx-auto px-4 sm:px-6 lg:px-8 py-6"><div class="flex flex-col sm:flex-row justify-between items-center gap-2"><p class="text-xs text-neutral-500">Last updated: <!-- -->November 30, 2025</p><p class="text-xs text-neutral-500 flex items-center"><a href="https://github.com/xyjoey/PRISM" target="_blank" rel="noopener noreferrer">Built with PRISM</a><span class="ml-2">üöÄ</span></p></div></div></footer></div><script src="/_next/static/chunks/webpack-679b75d1c4c2027c.js" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0])</script><script>self.__next_f.push([1,"1:\"$Sreact.fragment\"\n2:I[3719,[\"17\",\"static/chunks/17-9c04c9413a9f9f1f.js\",\"874\",\"static/chunks/874-6cc630662f3664af.js\",\"862\",\"static/chunks/862-15038af301665bb3.js\",\"177\",\"static/chunks/app/layout-347869ba10f90412.js\"],\"ThemeProvider\"]\n3:I[768,[\"17\",\"static/chunks/17-9c04c9413a9f9f1f.js\",\"874\",\"static/chunks/874-6cc630662f3664af.js\",\"862\",\"static/chunks/862-15038af301665bb3.js\",\"177\",\"static/chunks/app/layout-347869ba10f90412.js\"],\"default\"]\n4:I[7555,[],\"\"]\n5:I[1295,[],\"\"]\n6:I[167,[\"17\",\"static/chunks/17-9c04c9413a9f9f1f.js\",\"874\",\"static/chunks/874-6cc630662f3664af.js\",\"862\",\"static/chunks/862-15038af301665bb3.js\",\"177\",\"static/chunks/app/layout-347869ba10f90412.js\"],\"default\"]\n7:I[7437,[\"17\",\"static/chunks/17-9c04c9413a9f9f1f.js\",\"178\",\"static/chunks/178-595a94b9af1e67b5.js\",\"874\",\"static/chunks/874-6cc630662f3664af.js\",\"748\",\"static/chunks/748-1f3129a1e6365cf9.js\",\"974\",\"static/chunks/app/page-054ab306d65a82c3.js\"],\"default\"]\n8:I[9507,[\"17\",\"static/chunks/17-9c04c9413a9f9f1f.js\",\"178\",\"static/chunks/178-595a94b9af1e67b5.js\",\"874\",\"static/chunks/874-6cc630662f3664af.js\",\"748\",\"static/chunks/748-1f3129a1e6365cf9.js\",\"974\",\"static/chunks/app/page-054ab306d65a82c3.js\"],\"default\"]\n9:I[1990,[\"17\",\"static/chunks/17-9c04c9413a9f9f1f.js\",\"178\",\"static/chunks/178-595a94b9af1e67b5.js\",\"874\",\"static/chunks/874-6cc630662f3664af.js\",\"748\",\"static/chunks/748-1f3129a1e6365cf9.js\",\"974\",\"static/chunks/app/page-054ab306d65a82c3.js\"],\"default\"]\na:I[5218,[\"17\",\"static/chunks/17-9c04c9413a9f9f1f.js\",\"178\",\"static/chunks/178-595a94b9af1e67b5.js\",\"874\",\"static/chunks/874-6cc630662f3664af.js\",\"748\",\"static/chunks/748-1f3129a1e6365cf9.js\",\"974\",\"static/chunks/app/page-054ab306d65a82c3.js\"],\"default\"]\nb:I[9665,[],\"MetadataBoundary\"]\nd:I[9665,[],\"OutletBoundary\"]\n10:I[4911,[],\"AsyncMetadataOutlet\"]\n12:I[9665,[],\"ViewportBoundary\"]\n14:I[6614,[],\"\"]\n:HL[\"/_next/static/css/19bf7477d82f620e.css\",\"style\"]\n"])</script><script>self.__next_f.push([1,"0:{\"P\":null,\"b\":\"Iyb-ENhYiSc37zQj5YTEf\",\"p\":\"\",\"c\":[\"\",\"\"],\"i\":false,\"f\":[[[\"\",{\"children\":[\"__PAGE__\",{}]},\"$undefined\",\"$undefined\",true],[\"\",[\"$\",\"$1\",\"c\",{\"children\":[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/19bf7477d82f620e.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}]],[\"$\",\"html\",null,{\"lang\":\"en\",\"className\":\"scroll-smooth\",\"suppressHydrationWarning\":true,\"children\":[[\"$\",\"head\",null,{\"children\":[[\"$\",\"link\",null,{\"rel\":\"icon\",\"href\":\"/favicon.svg\",\"type\":\"image/svg+xml\"}],[\"$\",\"link\",null,{\"rel\":\"dns-prefetch\",\"href\":\"https://google-fonts.jialeliu.com\"}],[\"$\",\"link\",null,{\"rel\":\"preconnect\",\"href\":\"https://google-fonts.jialeliu.com\",\"crossOrigin\":\"\"}],[\"$\",\"link\",null,{\"rel\":\"preload\",\"as\":\"style\",\"href\":\"https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700\u0026family=Crimson+Text:ital,wght@0,400;0,600;1,400\u0026display=swap\"}],[\"$\",\"link\",null,{\"rel\":\"stylesheet\",\"id\":\"gfonts-css\",\"href\":\"https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700\u0026family=Crimson+Text:ital,wght@0,400;0,600;1,400\u0026display=swap\",\"media\":\"print\"}],[\"$\",\"script\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"\\n              (function(){\\n                var l = document.getElementById('gfonts-css');\\n                if (!l) return;\\n                if (l.media !== 'all') {\\n                  l.addEventListener('load', function(){ try { l.media = 'all'; } catch(e){} });\\n                }\\n              })();\\n            \"}}],[\"$\",\"noscript\",null,{\"children\":[\"$\",\"link\",null,{\"rel\":\"stylesheet\",\"href\":\"https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700\u0026family=Crimson+Text:ital,wght@0,400;0,600;1,400\u0026display=swap\"}]}],[\"$\",\"script\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"\\n              try {\\n                const theme = localStorage.getItem('theme-storage');\\n                const parsed = theme ? JSON.parse(theme) : null;\\n                const setting = parsed?.state?.theme || 'system';\\n                const prefersDark = typeof window !== 'undefined' \u0026\u0026 window.matchMedia \u0026\u0026 window.matchMedia('(prefers-color-scheme: dark)').matches;\\n                const effective = setting === 'dark' ? 'dark' : (setting === 'light' ? 'light' : (prefersDark ? 'dark' : 'light'));\\n                var root = document.documentElement;\\n                root.classList.add(effective);\\n                root.setAttribute('data-theme', effective);\\n              } catch (e) {\\n                var root = document.documentElement;\\n                root.classList.add('light');\\n                root.setAttribute('data-theme', 'light');\\n              }\\n            \"}}]]}],[\"$\",\"body\",null,{\"className\":\"font-sans antialiased\",\"children\":[\"$\",\"$L2\",null,{\"children\":[[\"$\",\"$L3\",null,{\"items\":[{\"title\":\"About\",\"type\":\"page\",\"target\":\"about\",\"href\":\"/\"},{\"title\":\"Publications\",\"type\":\"page\",\"target\":\"publications\",\"href\":\"/publications\"},{\"title\":\"Career\",\"type\":\"page\",\"target\":\"career\",\"href\":\"/career\"},{\"title\":\"Awards\",\"type\":\"page\",\"target\":\"awards\",\"href\":\"/awards\"},{\"title\":\"Services\",\"type\":\"page\",\"target\":\"services\",\"href\":\"/services\"}],\"siteTitle\":\"Zhengzhuo Xu (ËÆ∏Ê≠£Âçì)\",\"enableOnePageMode\":false}],[\"$\",\"main\",null,{\"className\":\"min-h-screen pt-16 lg:pt-20\",\"children\":[\"$\",\"$L4\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L5\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[[[\"$\",\"title\",null,{\"children\":\"404: This page could not be found.\"}],[\"$\",\"div\",null,{\"style\":{\"fontFamily\":\"system-ui,\\\"Segoe UI\\\",Roboto,Helvetica,Arial,sans-serif,\\\"Apple Color Emoji\\\",\\\"Segoe UI Emoji\\\"\",\"height\":\"100vh\",\"textAlign\":\"center\",\"display\":\"flex\",\"flexDirection\":\"column\",\"alignItems\":\"center\",\"justifyContent\":\"center\"},\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"style\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\"}}],[\"$\",\"h1\",null,{\"className\":\"next-error-h1\",\"style\":{\"display\":\"inline-block\",\"margin\":\"0 20px 0 0\",\"padding\":\"0 23px 0 0\",\"fontSize\":24,\"fontWeight\":500,\"verticalAlign\":\"top\",\"lineHeight\":\"49px\"},\"children\":404}],[\"$\",\"div\",null,{\"style\":{\"display\":\"inline-block\"},\"children\":[\"$\",\"h2\",null,{\"style\":{\"fontSize\":14,\"fontWeight\":400,\"lineHeight\":\"49px\",\"margin\":0},\"children\":\"This page could not be found.\"}]}]]}]}]],[]],\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]}],[\"$\",\"$L6\",null,{\"lastUpdated\":\"November 30, 2025\"}]]}]}]]}]]}],{\"children\":[\"__PAGE__\",[\"$\",\"$1\",\"c\",{\"children\":[[\"$\",\"div\",null,{\"className\":\"max-w-6xl mx-auto px-4 sm:px-6 lg:px-8 py-8 bg-background min-h-screen\",\"children\":[\"$\",\"div\",null,{\"className\":\"grid grid-cols-1 lg:grid-cols-3 gap-12\",\"children\":[[\"$\",\"div\",null,{\"className\":\"lg:col-span-1\",\"children\":[\"$\",\"$L7\",null,{\"author\":{\"name\":\"Zhengzhuo Xu\",\"title\":\"Tsinghua University\",\"institution\":\"CS Ph.D. Candidate\",\"avatar\":\"/bio.jpg\"},\"social\":{\"email\":\"xzzthu@gmail.com\",\"location\":\"Shenzhen, China\",\"location_url\":\"https://maps.google.com\",\"location_details\":[\"Room 123, Science Building,\",\"University Avenue, City, Country\"],\"google_scholar\":\"https://scholar.google.com/citations?hl=zh-CN\u0026user=08hV-X8AAAAJ\",\"orcid\":\"\",\"github\":\"https://github.com/XuZhengzhuo/\",\"linkedin\":\"\"},\"features\":{\"enable_likes\":true,\"enable_one_page_mode\":false},\"researchInterests\":[\"Perception and Reasoning\",\"Vision Language Models\",\"Long-tailed Visual Recognition\"]}]}],[\"$\",\"div\",null,{\"className\":\"lg:col-span-2 space-y-8\",\"children\":[[\"$\",\"section\",\"about\",{\"id\":\"about\",\"className\":\"scroll-mt-24 space-y-8\",\"children\":[[[\"$\",\"$L8\",\"about\",{\"content\":\"I am a CS Ph.D candidate at [SIGS](https://www.sigs.tsinghua.edu.cn/), [Tsinghua University](https://www.tsinghua.edu.cn/), under the supervision of *Prof*. [Yuan Chun](https://www.sigs.tsinghua.edu.cn/yc2/main.htm). I graduated with a Bachelor degree of Electrical Engineering from [Qiming College](http://qiming.hust.edu.cn/), [EIC](http://ei.hust.edu.cn/), [Huazhong University of Science and Technology](https://www.hust.edu.cn/) in **June 2020**, and I commenced my Ph.D studies at Tsinghua University in **September 2020**.\",\"title\":\"About\"}],[\"$\",\"$L9\",\"news\",{\"items\":[{\"date\":\"2025-07\",\"content\":\"Our work has been accepted by ICCV 2025 üéâ\"}],\"title\":\"News\"}],[\"$\",\"$La\",\"featured_publications\",{\"publications\":[{\"id\":\"ChartPoint\",\"title\":\"ChartPoint: Guiding MLLMs with Grounding Reflection for Chart Reasoning\",\"authors\":[{\"name\":\"Zhengzhuo Xu\",\"isHighlighted\":true,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Sinan Du\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Yiyan Qi\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Siwen Lu\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Chengjin Xu\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Chun Yuan\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Jian Guo\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false}],\"year\":2025,\"month\":\"9\",\"type\":\"journal\",\"status\":\"published\",\"tags\":[\"Chart Understanding\",\"Mixture of Experts\",\"MoE\",\"Visual Reasoning\",\"Multimodal Learning\"],\"keywords\":\"$0:f:0:1:2:children:1:props:children:0:props:children:props:children:1:props:children:0:props:children:0:2:props:publications:0:tags\",\"researchArea\":\"machine-learning\",\"journal\":\"IEEE/CVF International Conference on Computer Vision, ICCV\",\"conference\":\"\",\"volume\":\"pp 1--12\",\"abstract\":\"\",\"description\":\"A research on ChartMoE, a Mixture of Expert Connector framework designed to enhance advanced chart understanding capabilities, leveraging expert specialization to address complex visual and semantic reasoning challenges in chart data analysis.\",\"selected\":true,\"preview\":\"ChartPoint.png\",\"bibtex\":\"@article{ChartPoint,\\n  title = {ChartPoint: Guiding MLLMs with Grounding Reflection for Chart Reasoning},\\n  journal = {{IEEE/CVF} International Conference on Computer Vision, {ICCV}},\\n  volume = {pp 1--12},\\n  author = {Zhengzhuo Xu and Sinan Du and Yiyan Qi and Siwen Lu and Chengjin Xu and Chun Yuan and Jian Guo},\\n  year = {2025},\\n  month = {sep},\\n  publisher = {{IEEE}},\\n  abstract = {}\\n}\"},{\"id\":\"GuoWZLWSCFWWCJ25\",\"title\":\"Seed1.5-VL Technical Report\",\"authors\":[{\"name\":\"Dong Guo\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Faming Wu\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Feida Zhu\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Fuxing Leng\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Guang Shi\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Haobin Chen\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Haoqi Fan\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Jian Wang\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Jianyu Jiang\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Jiawei Wang\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Zhengzhuo Xu\",\"isHighlighted\":true,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"others\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false}],\"year\":2025,\"month\":\"5\",\"type\":\"journal\",\"status\":\"published\",\"tags\":[\"Multimodal Model\",\"Visual-Language Learning\",\"Seed1.5-VL\",\"Technical Report\",\"CoRR\",\"arXiv\"],\"keywords\":\"$0:f:0:1:2:children:1:props:children:0:props:children:props:children:1:props:children:0:props:children:0:2:props:publications:1:tags\",\"researchArea\":\"machine-learning\",\"journal\":\"CoRR\",\"conference\":\"\",\"volume\":\"abs/2505.07062\",\"doi\":\"10.48550/ARXIV.2505.07062\",\"abstract\":\"\",\"description\":\"A technical report on Seed1.5-VL, a multimodal model focusing on visual-language tasks, detailing the model architecture, training strategies, and performance evaluations across diverse multimodal benchmarks.\",\"selected\":true,\"preview\":\"Seed1.5VL.png\",\"bibtex\":\"@article{GuoWZLWSCFWWCJ25,\\n  title = {Seed1.5-VL Technical Report},\\n  journal = {CoRR},\\n  volume = {abs/2505.07062},\\n  author = {Dong Guo and Faming Wu and Feida Zhu and Fuxing Leng and Guang Shi and Haobin Chen and Haoqi Fan and Jian Wang and Jianyu Jiang and Jiawei Wang and Zhengzhuo Xu and others},\\n  year = {2025},\\n  month = {may},\\n  publisher = {arXiv},\\n  address = {Cornell University Library},\\n  doi = {10.48550/ARXIV.2505.07062},\\n  urldate = {2025-11-14},\\n  issn = {1932-4553},\\n  eprinttype = {arXiv},\\n  eprint = {2505.07062},\\n  abstract = {}\\n}\"},{\"id\":\"XuQQDXYG24\",\"title\":\"ChartMoE: Mixture of Expert Connector for Advanced Chart Understanding\",\"authors\":[{\"name\":\"Zhengzhuo Xu\",\"isHighlighted\":true,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Bowen Qu\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Yiyan Qi\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Sinan Du\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Chengjin Xu\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Chun Yuan\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Jian Guo\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false}],\"year\":2024,\"month\":\"9\",\"type\":\"journal\",\"status\":\"published\",\"tags\":[\"Chart Understanding\",\"Mixture of Experts\",\"MoE\",\"Visual Reasoning\",\"Multimodal Learning\"],\"keywords\":\"$0:f:0:1:2:children:1:props:children:0:props:children:props:children:1:props:children:0:props:children:0:2:props:publications:2:tags\",\"researchArea\":\"machine-learning\",\"journal\":\"International Conference on Learning Representations, ICLR oral\",\"conference\":\"\",\"volume\":\"pp 1--12\",\"abstract\":\"\",\"description\":\"A research on ChartMoE, a Mixture of Expert Connector framework designed to enhance advanced chart understanding capabilities, leveraging expert specialization to address complex visual and semantic reasoning challenges in chart data analysis.\",\"selected\":true,\"preview\":\"ChartMoE.png\",\"bibtex\":\"@article{XuQQDXYG24,\\n  title = {ChartMoE: Mixture of Expert Connector for Advanced Chart Understanding},\\n  journal = {International Conference on Learning Representations, ICLR oral},\\n  volume = {pp 1--12},\\n  author = {Zhengzhuo Xu and Bowen Qu and Yiyan Qi and Sinan Du and Chengjin Xu and Chun Yuan and Jian Guo},\\n  year = {2024},\\n  month = {sep},\\n  publisher = {OpenReview.net},\\n  urldate = {2024-10-09},\\n  issn = {1932-4553},\\n  abstract = {}\\n}\"},{\"id\":\"XuLYCY23\",\"title\":\"Learning Imbalanced Data with Vision Transformers\",\"authors\":[{\"name\":\"Zhengzhuo Xu\",\"isHighlighted\":true,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Ruikang Liu\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Shuo Yang\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Zenghao Chai\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Chun Yuan\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false}],\"year\":2023,\"month\":\"6\",\"type\":\"conference\",\"status\":\"published\",\"tags\":[\"Imbalanced Data Learning\",\"Vision Transformer\",\"ViT\",\"Class Imbalance\"],\"keywords\":\"$0:f:0:1:2:children:1:props:children:0:props:children:props:children:1:props:children:0:props:children:0:2:props:publications:3:tags\",\"researchArea\":\"transformer-architectures\",\"journal\":\"\",\"conference\":\"IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR\",\"pages\":\"15793--15803\",\"doi\":\"10.1109/CVPR52729.2023.01516\",\"abstract\":\"\",\"description\":\"A research on learning imbalanced data with Vision Transformer models, proposing novel strategies to address the class imbalance challenge in computer vision tasks using ViT architectures.\",\"selected\":true,\"preview\":\"LiVT.png\",\"bibtex\":\"@inproceedings{XuLYCY23,\\n  title = {Learning Imbalanced Data with Vision Transformers},\\n  booktitle = {{IEEE/CVF} Conference on Computer Vision and Pattern Recognition, {CVPR}},\\n  author = {Zhengzhuo Xu and Ruikang Liu and Shuo Yang and Zenghao Chai and Chun Yuan},\\n  year = {2023},\\n  month = {jun},\\n  pages = {15793--15803},\\n  publisher = {{IEEE}},\\n  address = {Vancouver, BC, Canada},\\n  doi = {10.1109/CVPR52729.2023.01516},\\n  urldate = {2025-11-26},\\n  isbn = {978-1-6654-8818-8},\\n  abstract = {}\\n}\"},{\"id\":\"XuCY21\",\"title\":\"Towards Calibrated Model for Long-Tailed Visual Recognition from Prior Perspective\",\"authors\":[{\"name\":\"Zhengzhuo Xu\",\"isHighlighted\":true,\"isCorresponding\":false,\"isCoAuthor\":true},{\"name\":\"Zenghao Chai\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false},{\"name\":\"Chun Yuan\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false}],\"year\":2021,\"month\":\"12\",\"type\":\"conference\",\"status\":\"published\",\"tags\":[\"Long-Tailed Recognition\",\"Visual Recognition\",\"Model Calibration\",\"Prior Knowledge\",\"Deep Learning\"],\"keywords\":\"$0:f:0:1:2:children:1:props:children:0:props:children:props:children:1:props:children:0:props:children:0:2:props:publications:4:tags\",\"researchArea\":\"machine-learning\",\"journal\":\"\",\"conference\":\"Advances in Neural Information Processing Systems, NeurIPS\",\"pages\":\"7139--7152\",\"doi\":\"10.48550/arXiv.2110.04456\",\"abstract\":\"\",\"description\":\"A research on calibrated models for long-tailed visual recognition from a prior perspective, addressing the calibration and classification challenges in long-tailed visual data scenarios.\",\"selected\":true,\"preview\":\"NIPS2021.png\",\"bibtex\":\"@inproceedings{XuCY21,\\n  title = {Towards Calibrated Model for Long-Tailed Visual Recognition from Prior Perspective},\\n  booktitle = {Advances in Neural Information Processing Systems, NeurIPS},\\n  editor = {Marc'Aurelio Ranzato and Alina Beygelzimer and Yann N. Dauphin and Percy Liang and Jennifer Wortman Vaughan},\\n  author = {Zhengzhuo Xu and Zenghao Chai and Chun Yuan},\\n  year = {2021},\\n  month = {dec},\\n  pages = {7139--7152},\\n  publisher = {Curran Associates, Inc.},\\n  address = {Virtual Event},\\n  doi = {10.48550/arXiv.2110.04456},\\n  urldate = {2022-05-03},\\n  isbn = {978-1-7138-4539-3},\\n  abstract = {}\\n}\"}],\"title\":\"Selected Publications\",\"enableOnePageMode\":true}]],false,false,false]}]]}]]}]}],[\"$\",\"$Lb\",null,{\"children\":\"$Lc\"}],null,[\"$\",\"$Ld\",null,{\"children\":[\"$Le\",\"$Lf\",[\"$\",\"$L10\",null,{\"promise\":\"$@11\"}]]}]]}],{},null,false]},null,false],[\"$\",\"$1\",\"h\",{\"children\":[null,[\"$\",\"$1\",\"3kxR07aQI0O16IgHaUFNv\",{\"children\":[[\"$\",\"$L12\",null,{\"children\":\"$L13\"}],null]}],null]}],false]],\"m\":\"$undefined\",\"G\":[\"$14\",\"$undefined\"],\"s\":false,\"S\":true}\n"])</script><script>self.__next_f.push([1,"15:\"$Sreact.suspense\"\n16:I[4911,[],\"AsyncMetadata\"]\nc:[\"$\",\"$15\",null,{\"fallback\":null,\"children\":[\"$\",\"$L16\",null,{\"promise\":\"$@17\"}]}]\n"])</script><script>self.__next_f.push([1,"f:null\n"])</script><script>self.__next_f.push([1,"13:[[\"$\",\"meta\",\"0\",{\"charSet\":\"utf-8\"}],[\"$\",\"meta\",\"1\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}]]\ne:null\n"])</script><script>self.__next_f.push([1,"17:{\"metadata\":[[\"$\",\"title\",\"0\",{\"children\":\"Zhengzhuo Xu (ËÆ∏Ê≠£Âçì)\"}],[\"$\",\"meta\",\"1\",{\"name\":\"description\",\"content\":\"PhD student at the University of Example.\"}],[\"$\",\"meta\",\"2\",{\"name\":\"author\",\"content\":\"Zhengzhuo Xu\"}],[\"$\",\"meta\",\"3\",{\"name\":\"keywords\",\"content\":\"Zhengzhuo Xu,PhD,Research,CS Ph.D. Candidate\"}],[\"$\",\"meta\",\"4\",{\"name\":\"creator\",\"content\":\"Zhengzhuo Xu\"}],[\"$\",\"meta\",\"5\",{\"name\":\"publisher\",\"content\":\"Zhengzhuo Xu\"}],[\"$\",\"meta\",\"6\",{\"property\":\"og:title\",\"content\":\"Zhengzhuo Xu (ËÆ∏Ê≠£Âçì)\"}],[\"$\",\"meta\",\"7\",{\"property\":\"og:description\",\"content\":\"PhD student at the University of Example.\"}],[\"$\",\"meta\",\"8\",{\"property\":\"og:site_name\",\"content\":\"Zhengzhuo Xu's Academic Website\"}],[\"$\",\"meta\",\"9\",{\"property\":\"og:locale\",\"content\":\"en_US\"}],[\"$\",\"meta\",\"10\",{\"property\":\"og:type\",\"content\":\"website\"}],[\"$\",\"meta\",\"11\",{\"name\":\"twitter:card\",\"content\":\"summary\"}],[\"$\",\"meta\",\"12\",{\"name\":\"twitter:title\",\"content\":\"Zhengzhuo Xu (ËÆ∏Ê≠£Âçì)\"}],[\"$\",\"meta\",\"13\",{\"name\":\"twitter:description\",\"content\":\"PhD student at the University of Example.\"}],[\"$\",\"link\",\"14\",{\"rel\":\"icon\",\"href\":\"/favicon.svg\"}]],\"error\":null,\"digest\":\"$undefined\"}\n11:{\"metadata\":\"$17:metadata\",\"error\":null,\"digest\":\"$undefined\"}\n"])</script></body></html>